\chapter{Contexto}\label{ch:contexto}

En esta parte vamos a analizar la convergencia de algoritmos estoc\'asticos para optimizar una $F: \R^d \mapsto \R$ que puede representar tanto el costo esperado como el emp\'irico. Recordemos que $F$ lo asumimos parametrizado por$w \in \R^d$ e imaginamos a los datos $(x,y)$ como extra\'idos de una variable aleatoria $\upxi$ cuya distribuci\'on desconocida es $P$, luego $F$ se representa como:

\begin{equation}
F(w) = \left\lbrace
\begin{aligned}
R(w) & = & \expectation{f(w, \upxi)} \\
& \text{o} & \\
R_n(w) & = & \frac{1}{n} \sum\limits_{i=1}^{n} {f_i(w)}
\end{aligned}
\right.
\end{equation}

Sea el algoritmo estoc\'astico \ref{algo: DE}

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[H]
	\caption{Descenso Estocastico (DE) \label{algo: DE}}
	\textbf{Input:} $w_1 \in \R^d$ el inicio de la iteraci\'on, $\sett{\upxi_{k}} $ iid \\
	\For{$k \in \N$}{
		Generar una muestra de la variable aleatoria $\upxi_k$ \\
		Calcular el vector estoc\'astico $g(w_k, \upxi_k)$ \\
		Elegir $\alpha_k >0$ \\
		$w_{k+1} \leftarrow w_k - \alpha_kg(w_k, \upxi_k)$
	}
\end{algorithm}

Notemos que representa en forma general los algoritmos estoc\'asticos mas comunes. En particular, una muestra de $\upxi_k$ puede ser un \'unico par $(x_i, y_i)$ como en el \textit{Descenso por gradiente estoc\'astico} o una muestra $S_n = \sett{(x_i, y_i)}_{i \leq n}$ como en \textit{Mini-Batch Descenso por gradiente estoc\'astico} ; a su vez, $g(w_k, \upxi_k)$ puede ser varias estimaciones del gradiente como por ejemplo:

\begin{equation}
g(w_k, \upxi_k) = \left\lbrace
\begin{aligned}
\nabla f(w_k, \upxi_k) \\
\frac{1}{n_k} \sum\limits_{i=1}^{n_k} {\nabla f (w_k, \upxi_{k,i})}\\
H_k\frac{1}{n_k} \sum\limits_{i=1}^{n_k} {\nabla f (w_k, \upxi_{k,i})}
\end{aligned}
\right.
\end{equation}

Donde $H_k$ es una matriz sim\'etrica definida positiva como en los m\'etodos de Newton-Gauss. 

\smallskip

Para iniciar el analisis de la convergencia, lo m\'inimo que necesitamos es que el gradiente se mantenga controlado, por lo tanto asumamos:

\begin{hyp} [$F$ es $l$-Lipshitz]
	\label{hyp: F es L lipshitz}
	La funci\'on a optimizar $F \in C^1(\R^d)$ y existe $L >0$ tal que para todos $w,z \in \R^d$:
	\begin{equation*}
		\norm{\nabla F(w) - \nabla F(z)}_2 \leq L \norm{w-z}_2
	\end{equation*} 
\end{hyp}

\begin{remark}
	\label{obs: F es l lipshitz}
	Sea $F$ bajo \ref{hyp: F es L lipshitz}, luego para todos $w,z\in \R^d$ vale:
	
	\begin{equation*}
		F(w) \leq F(z) + \nabla F(z)^T(w-z) + \frac{1}{2}L \norm{w-z}^2_2
	\end{equation*}

\end{remark}

\begin{proof}
	Notemos que:
	
	\begin{equation*}
	\begin{aligned}[l]
	F(w) & = & F(z) + \int\limits_{0}^{1} {\dfrac{\partial F \left(z + t(w-z)\right)}{\partial t}}dt \\
	& = & F(z) + \int\limits_{0}^{1} {\nabla F \left(z + t(w-z)\right)^T (w-z)}dt \\
	& = & F(z) + \nabla F(z)^T(w-z) + \int\limits_{0}^{1} { \left[\nabla F \left(z + t(w-z)\right) - \nabla F(z)\right]^T (w-z)}dt \\
	& \leq & F(z) + \nabla F(z)^T(w-z) + \int\limits_{0}^{1} { L \norm{t(w-z)}_2 \norm{w-z}_2}dt \\
	& = & F(z) + \nabla F(z)^T(w-z) + \frac{1}{2}L \norm{w-z}^2_2. \qed
	\end{aligned}
	\end{equation*}
	
\end{proof}

Definamos ahora $\expectationsub{\upxi_k}{.} := \mathbb{E}_{P_k}\left[. \vert w_k\right]$ la esperanza condicional bajo la distribuci\'on de $\upxi_k$ dado $w_k$.

\begin{lemma}
	\label{lemma: Fundamental estocasticos 1}
	Bajo \ref{hyp: F es L lipshitz} las iteraciones de \ref{algo: DE} satisfacen que para todo $k \in N$:
	
	\begin{equation}
		\expectationsub{\upxi_{k}}{F(w_{k+1})} - F(w_k) \leq - \alpha_k \nabla F(w_k) ^T \expectationsub{\upxi_{k}}{g(w_k, \upxi_{k})} + \frac{1}{2} \alpha_k^2 \expectationsub{\upxi_{k}}{\norm{g(w_k, \upxi_{k})}^2_2}
	\end{equation}
	
\end{lemma}

\begin{proof}
	Notemos que por \ref{hyp: F es L lipshitz} vale que:
	
	\begin{equation*}
		\begin{aligned}
			F(w_{k+1}) - F(w_k) & \leq &  \nabla F(w_k)^T(w{k+1}-w_k) + \frac{1}{2}L \norm{w{k+1}-w_k}^2_2 \\
			& \leq & - \alpha_k \nabla F(w_k)^Tg(w_k, \upxi_{k}) + \frac{1}{2} \alpha_k^2 L \norm{g(w_k, \upxi_{k})}^2_2 
		\end{aligned}
	\end{equation*}
	
	\marginpar{Aca usamos propiedades basicas de la esperanza condicional}
	
	Luego tomando esperanza de ambos lados y recordando que si $X,Y$ son independientes entonces $\expectationsub{X,Y}{Y \vert X} = \expectation{Y}$:
	
	\begin{equation*}
		\begin{aligned}
				\expectationchik{F(w_{k+1}) - F(w_k)} & \leq & - \alpha_k \expectationchik{\nabla F(w_k)^Tg(w_k, \upxi_{k})} + \frac{1}{2} \alpha_k^2 L \expectationchik{\norm{g(w_k, \upxi_{k})}^2_2 } \\
				\expectationchik{F(w_{k+1})} - F(w_k) & \leq & - \alpha_k \nabla F(w_k)^T \expectationchik{g(w_k, \upxi_{k})}+ \frac{1}{2} \alpha_k^2 L \expectationchik{\norm{g(w_k, \upxi_{k})}^2_2 } 
		\end{aligned}
	\end{equation*}
	\qed
	
\end{proof}

\begin{remark}
	Notemos que si $g(w_k, \upxi_{k})$ es un estimador insesgado de $\nabla F (w_k)$ entonces de \ref{lemma: Fundamental estocasticos 1}:
	
		\begin{equation}
	\expectationsub{\upxi_{k}}{F(w_{k+1})} - F(w_k) \leq - \alpha_k \norm{\nabla F(w_k)}^2 + \frac{1}{2} \alpha_k^2 \expectationsub{\upxi_{k}}{\norm{g(w_k, \upxi_{k})}^2_2}
	\end{equation}
	
\end{remark}

Luego entonces para controlar la convergencia de \ref{algo: DE} tambi\'en hay que poner suposiciones sobre el segundo momento de $g$, luego si definimos:


\begin{equation}
\label{def: Definicion varianza condicional}
\variancechik{g(w_k, \upxi_{k})} := \expectationchik{\norm{g(w_k, \upxi_{k})}_2^2} - \norm{\expectationchik{g(w_k, \upxi_{k})}}_2^2
\end{equation}

Asumamos:

\begin{hyp}[Acotaciones al primer y segundo momento de $g$]
	\label{hyp: Acotaciones momentos de g}
	Supongamos que dada $F$ funci\'on objetivo y $g$ la estimaci\'on del gradiente en \ref{algo: DE} vale:
	
	\begin{enumerate}
		\item Existe $U \subset \R^d$ tal que $\sett{w_k} \subset U$ y que existe $F_{inf}$ tal que $F\vert_U \geq F_{inf}$
		\item Existen $\mu_G \geq \mu \geq 0$ tal que para todo $k \in N$ valen:
		
		\begin{subequations}
		\begin{equation}
		\label{eq: Hipotesis 2 subitem 1}
		\nabla F(w_k)^T \expectationchik{g(w_k, \upxi_{k})} \geq \mu \norm{\nabla F(w_k)}_2^2
		\end{equation}
		Y
		\begin{equation}
		\label{eq: Hipotesis 2 subitem 2}
		\norm{\expectationchik{g(w_k, \upxi_{k})}}_2 \leq \mu_G \norm{\nabla F(w_k)}_2
		\end{equation}
		\end{subequations}
		\item 	Existen $M, M_V \geq 0$ tal que para todo $k \in \N$:
		\begin{equation}
		\label{eq: Hipotesis 2 Segundo momento acotado}
		\variancechik{g(w_k, \upxi_{k})} \leq M + M_V \norm{\nabla F (w_k)}_2^2
		\end{equation}
		
		
	\end{enumerate}
	
\end{hyp}

\begin{remark}
	Notemos que si $g$ es un estimador insesgado de $\nabla F$ entonces \ref{eq: Hipotesis 2 subitem 1} y \ref{eq: Hipotesis 2 subitem 2} valen con $\mu_G = \mu = 1$. Dejamos de ejercicio al lector notar que si $H_k$ es sim\'etrica positiva definida tal que $H_k$ es independiente de $\upxi_{k}$ entonces tanto \ref{eq: Hipotesis 2 subitem 1} como \ref{eq: Hipotesis 2 subitem 2} valen.
\end{remark}

\begin{remark}
	\label{Obs: Acotaciones momentos de g}
	Bajo \ref{hyp: Acotaciones momentos de g} y por \ref{def: Definicion varianza condicional} tenemos que:
	
	\begin{equation*}
	\begin{aligned}
		\expectationchik{\norm{g(w_k, \upxi_{k})}_2^2} & \leq & \norm{\expectationchik{g(w_k, \upxi_{k})}}_2^2 + M + M_V\norm{\nabla F(w_k)}_2^2 \\
		& \leq & M + M_G\norm{\nabla F(w_k)}_2^2 \\
	\end{aligned}
	\end{equation*}
	$M_G:= M_V + \mu_G^2 \geq \mu^2 \geq 0$
\end{remark}

\begin{lemma}
	\label{lemma: Fundamental estocasticos 2}
	Bajo \ref{hyp: Acotaciones momentos de g} y \ref{hyp: F es L lipshitz} las iteraciones de \ref{algo: DE} satisfacen para todo $k \in \N$:
	
	\begin{subequations}
	\begin{equation}
	\label{eq: Fundamental estocasticos 2 ecuacion 1}
	\expectationchik{F(w_{k+1})} - F(w_k) \leq -\mu \alpha_k \norm{\nabla F(w_k)}_2^2 + \frac{1}{2} \alpha_k^2 L \expectationchik{\norm{g(w_k, \upxi_{k})}_2^2}
	\end{equation}
		\begin{equation}
	\label{eq: Fundamental estocasticos 2 ecuacion 2}
	\expectationchik{F(w_{k+1})} - F(w_k) \leq - \left( \mu - \frac{1}{2} \alpha_k L M_G \right)\alpha_k \norm{\nabla F(w_k)}_2^2 + \frac{1}{2} \alpha_k^2 L M
	\end{equation}
	\end{subequations}
	
\end{lemma}

\begin{proof}
	Por \ref{lemma: Fundamental estocasticos 1} y \ref{eq: Hipotesis 2 subitem 1} vale que:
	
	\begin{equation*}
	\begin{aligned}
	\expectationsub{\upxi_{k}}{F(w_{k+1})} - F(w_k) & \leq & - \alpha_k \nabla F(w_k) ^T \expectationsub{\upxi_{k}}{g(w_k, \upxi_{k})} + \frac{1}{2}L \alpha_k^2 \expectationsub{\upxi_{k}}{\norm{g(w_k, \upxi_{k})}^2_2} \\
	\expectationchik{F(w_{k+1})} - F(w_k) & \leq & -\mu \alpha_k \norm{\nabla F(w_k)}_2^2 + \frac{1}{2} \alpha_k^2 L \expectationchik{\norm{g(w_k, \upxi_{k})}_2^2}
	\end{aligned}
	\end{equation*}
	
	Que es \ref{eq: Fundamental estocasticos 2 ecuacion 1}; luego por \ref{Obs: Acotaciones momentos de g} obtenemos \ref{eq: Fundamental estocasticos 2 ecuacion 2}. \qed	
\end{proof}

\begin{corollary}
	\label{coro: DE es una cadena de markov}
	Bajo \ref{hyp: Acotaciones momentos de g} y \ref{hyp: F es L lipshitz} las iteraciones de \ref{algo: DE} satisfacen para todo $k \in \N$ que $\sett{w_k}$ es una cadena de Markov de primer orden.
\end{corollary}