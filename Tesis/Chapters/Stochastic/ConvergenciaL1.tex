\chapter{Convergencia en L1}\label{ch:convergenciaL1}

\epigraph{El axioma de elecci\'on es obviamente cierto, el principio de buen ordenamiento es obviamente falso y quien puede decidir sobre el lema de Zorn?}{Jerry Bona}

\section{Contexto}

En esta parte vamos a analizar la convergencia en L1 de algoritmos estoc\'asticos para optimizar una $F: \R^d \mapsto \R$ que puede representar tanto el costo esperado como el emp\'irico. Recordemos que $F$ lo asumimos parametrizado por $w \in \R^d$ e imaginamos los datos $(x,y)$ como extra\'idos de una variable aleatoria $\upxi$ cuya distribuci\'on desconocida es $P$, luego $F$ se representa como:

\begin{equation}
F(w) = \left\lbrace
\begin{aligned}
R(w) & = & \expectation{f(w, \upxi)} \\
& \text{o} & \\
R_n(w) & = & \frac{1}{n} \sum\limits_{i=1}^{n} {f_i(w)}
\end{aligned}
\right.
\end{equation}

Sea el algoritmo estoc\'astico \ref{algo: DE}

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[H]
	\caption{Descenso Estocastico (DE) \label{algo: DE}}
	\textbf{Input:} $w_1 \in \R^d$ el inicio de la iteraci\'on, $\sett{\upxi_{k}} $ iid \\
	\For{$k \in \N$}{
		Generar una muestra de la variable aleatoria $\upxi_k$ \\
		Calcular el vector estoc\'astico $g(w_k, \upxi_k)$ \\
		Elegir $\alpha_k >0$ \\
		$w_{k+1} \leftarrow w_k - \alpha_kg(w_k, \upxi_k)$
	}
\end{algorithm}

Notemos que representa en forma general los algoritmos estoc\'asticos mas comunes. En particular, una muestra de $\upxi_k$ puede ser un \'unico par $(x_i, y_i)$ como en el \textit{Descenso por gradiente estoc\'astico} o una muestra $S_n = \sett{(x_i, y_i)}_{i \leq n}$ como en \textit{mini-batch descenso por gradiente estoc\'astico} ; a su vez, $g(w_k, \upxi_k)$ puede ser varias estimaciones del gradiente como por ejemplo:

\begin{equation}
g(w_k, \upxi_k) = \left\lbrace
\begin{aligned}
\nabla f(w_k, \upxi_k) \\
\frac{1}{n_k} \sum\limits_{i=1}^{n_k} {\nabla f (w_k, \upxi_{k,i})}\\
H_k\frac{1}{n_k} \sum\limits_{i=1}^{n_k} {\nabla f (w_k, \upxi_{k,i})}
\end{aligned}
\right.
\end{equation}

Donde $H_k$ es una matriz sim\'etrica definida positiva como en los m\'etodos de Newton-Gauss. 

\smallskip

Para iniciar el analisis de la convergencia, lo m\'inimo que necesitamos es que el gradiente se mantenga controlado, por lo tanto recordemos la condici\'on \ref{def: L Lipschitz}:

\begin{hyp} [$F$ es $l$-Lipshitz]
	\label{hyp: F es L lipshitz}
	La funci\'on a optimizar $F: \R^d \rightarrow \R$ es de clase $C^1$ y existe $L >0$ tal que para todos $w,z \in \R^d$:
	\begin{equation*}
	\norm{\nabla F(w) - \nabla F(z)}_2 \leq L \norm{w-z}_2
	\end{equation*} 
\end{hyp}

\begin{remark}
	\label{obs: F es l lipshitz}
	Sea $F$ bajo \ref{hyp: F es L lipshitz}, luego para todos $w,z\in \R^d$ vale:
	
	\begin{equation*}
	F(w) \leq F(z) + \nabla F(z)^T(w-z) + \frac{1}{2}L \norm{w-z}^2_2
	\end{equation*}
	
\end{remark}

\begin{proof}
	Ver \ref{prop: Implicancias L Lipschitz}
\end{proof}

\section{Algunos lemas fundamentales}

Con el contexto claro, veamos algunos lemas que van a ser clave en la demostraci\'on de la convergencia L1 del algoritmo \ref{algo: DE}.

Definamos ahora $\expectationsub{\upxi_k}{.} := \mathbb{E}_{P_k}\left[. \vert w_k\right]$ la esperanza condicional bajo la distribuci\'on de $\upxi_k$ dado $w_k$.

\begin{lemma}
	\label{lemma: Fundamental estocasticos 1}
	Bajo \ref{hyp: F es L lipshitz} las iteraciones de \ref{algo: DE} satisfacen que para todo $k \in N$:
	
	\begin{equation}
	\expectationsub{\upxi_{k}}{F(w_{k+1})} - F(w_k) \leq - \alpha_k \nabla F(w_k) ^T \expectationsub{\upxi_{k}}{g(w_k, \upxi_{k})} + \frac{1}{2} \alpha_k^2 \expectationsub{\upxi_{k}}{\norm{g(w_k, \upxi_{k})}^2_2}
	\end{equation}
	
\end{lemma}

\begin{proof}
	Notemos que por \ref{hyp: F es L lipshitz} vale que:
	
	\begin{equation*}
	\begin{aligned}
	F(w_{k+1}) - F(w_k) & \leq &  \nabla F(w_k)^T(w{k+1}-w_k) + \frac{1}{2}L \norm{w{k+1}-w_k}^2_2 \\
	& \leq & - \alpha_k \nabla F(w_k)^Tg(w_k, \upxi_{k}) + \frac{1}{2} \alpha_k^2 L \norm{g(w_k, \upxi_{k})}^2_2 
	\end{aligned}
	\end{equation*}
	
	Luego tomando esperanza de ambos lados y recordando \ref{theorem: Propiedades de esperanza condicional}:
	
	\begin{equation*}
	\begin{aligned}
	\expectationchik{F(w_{k+1}) - F(w_k)} & \leq & - \alpha_k \expectationchik{\nabla F(w_k)^Tg(w_k, \upxi_{k})} + \frac{1}{2} \alpha_k^2 L \expectationchik{\norm{g(w_k, \upxi_{k})}^2_2 } \\
	\expectationchik{F(w_{k+1})} - F(w_k) & \leq & - \alpha_k \nabla F(w_k)^T \expectationchik{g(w_k, \upxi_{k})}+ \frac{1}{2} \alpha_k^2 L \expectationchik{\norm{g(w_k, \upxi_{k})}^2_2 } 
	\end{aligned}
	\end{equation*}
	\qed
	
\end{proof}

\begin{remark}
	Notemos que si $g(w_k, \upxi_{k})$ es un estimador insesgado de $\nabla F (w_k)$ entonces de \ref{lemma: Fundamental estocasticos 1}:
	
	\begin{equation}
	\expectationsub{\upxi_{k}}{F(w_{k+1})} - F(w_k) \leq - \alpha_k \norm{\nabla F(w_k)}^2 + \frac{1}{2} \alpha_k^2 \expectationsub{\upxi_{k}}{\norm{g(w_k, \upxi_{k})}^2_2}
	\end{equation}
	
\end{remark}

Luego entonces para controlar la convergencia de \ref{algo: DE} tambi\'en hay que poner suposiciones sobre el segundo momento de $g$, luego si definimos:


\begin{equation}
\label{def: Definicion varianza condicional}
\variancechik{g(w_k, \upxi_{k})} := \expectationchik{\norm{g(w_k, \upxi_{k})}_2^2} - \norm{\expectationchik{g(w_k, \upxi_{k})}}_2^2
\end{equation}

Asumamos:

\begin{hyp}[Acotaciones al primer y segundo momento de $g$]
	\label{hyp: Acotaciones momentos de g}
	Supongamos que dada $F$ funci\'on objetivo y $g$ la estimaci\'on del gradiente en \ref{algo: DE} vale:
	
	\begin{enumerate}
		\item Existe $U \subset \R^d$ tal que $\sett{w_k} \subset U$ y que existe $F_{inf}$ tal que $F\vert_U \geq F_{inf}$
		\item Existen $\mu_G \geq \mu \geq 0$ tal que para todo $k \in N$ valen:
		
		\begin{subequations}
			\begin{equation}
			\label{eq: Hipotesis 2 subitem 1}
			\nabla F(w_k)^T \expectationchik{g(w_k, \upxi_{k})} \geq \mu \norm{\nabla F(w_k)}_2^2
			\end{equation}
			Y
			\begin{equation}
			\label{eq: Hipotesis 2 subitem 2}
			\norm{\expectationchik{g(w_k, \upxi_{k})}}_2 \leq \mu_G \norm{\nabla F(w_k)}_2
			\end{equation}
		\end{subequations}
		\item 	Existen $M, M_V \geq 0$ tal que para todo $k \in \N$:
		\begin{equation}
		\label{eq: Hipotesis 2 Segundo momento acotado}
		\variancechik{g(w_k, \upxi_{k})} \leq M + M_V \norm{\nabla F (w_k)}_2^2
		\end{equation}
		
		
	\end{enumerate}
	
\end{hyp}

\begin{remark}
	Notemos que si $g$ es un estimador insesgado de $\nabla F$ entonces \ref{eq: Hipotesis 2 subitem 1} y \ref{eq: Hipotesis 2 subitem 2} valen con $\mu_G = \mu = 1$.
\end{remark}

\begin{remark}
	\label{Obs: Acotaciones momentos de g}
	Bajo \ref{hyp: Acotaciones momentos de g} y por \ref{def: Definicion varianza condicional} tenemos que:
	
	\begin{equation*}
	\begin{aligned}
	\expectationchik{\norm{g(w_k, \upxi_{k})}_2^2} & \leq & \norm{\expectationchik{g(w_k, \upxi_{k})}}_2^2 + M + M_V\norm{\nabla F(w_k)}_2^2 \\
	& \leq & M + M_G\norm{\nabla F(w_k)}_2^2 \\
	\end{aligned}
	\end{equation*}
	$M_G:= M_V + \mu_G^2 \geq \mu^2 \geq 0$
\end{remark}

\begin{lemma}
	\label{lemma: Fundamental estocasticos 2}
	Bajo \ref{hyp: Acotaciones momentos de g} y \ref{hyp: F es L lipshitz} las iteraciones de \ref{algo: DE} satisfacen para todo $k \in \N$:
	
	\begin{subequations}
		\begin{equation}
		\label{eq: Fundamental estocasticos 2 ecuacion 1}
		\expectationchik{F(w_{k+1})} - F(w_k) \leq -\mu \alpha_k \norm{\nabla F(w_k)}_2^2 + \frac{1}{2} \alpha_k^2 L \expectationchik{\norm{g(w_k, \upxi_{k})}_2^2}
		\end{equation}
		\begin{equation}
		\label{eq: Fundamental estocasticos 2 ecuacion 2}
		\expectationchik{F(w_{k+1})} - F(w_k) \leq - \left( \mu - \frac{1}{2} \alpha_k L M_G \right)\alpha_k \norm{\nabla F(w_k)}_2^2 + \frac{1}{2} \alpha_k^2 L M
		\end{equation}
	\end{subequations}
	
\end{lemma}

\begin{proof}
	Por \ref{lemma: Fundamental estocasticos 1} y \ref{eq: Hipotesis 2 subitem 1} vale que:
	
	\begin{equation*}
	\begin{aligned}
	\expectationsub{\upxi_{k}}{F(w_{k+1})} - F(w_k) & \leq & - \alpha_k \nabla F(w_k) ^T \expectationsub{\upxi_{k}}{g(w_k, \upxi_{k})} + \frac{1}{2}L \alpha_k^2 \expectationsub{\upxi_{k}}{\norm{g(w_k, \upxi_{k})}^2_2} \\
	\expectationchik{F(w_{k+1})} - F(w_k) & \leq & -\mu \alpha_k \norm{\nabla F(w_k)}_2^2 + \frac{1}{2} \alpha_k^2 L \expectationchik{\norm{g(w_k, \upxi_{k})}_2^2}
	\end{aligned}
	\end{equation*}
	
	Que es \ref{eq: Fundamental estocasticos 2 ecuacion 1}; luego por \ref{Obs: Acotaciones momentos de g} obtenemos \ref{eq: Fundamental estocasticos 2 ecuacion 2}. \qed	
\end{proof}

\begin{corollary}
	\label{coro: DE es una cadena de markov}
	Bajo \ref{hyp: Acotaciones momentos de g} y \ref{hyp: F es L lipshitz} las iteraciones de \ref{algo: DE} satisfacen para todo $k \in \N$ que $\sett{w_k}$ es una cadena de Markov de primer orden.
\end{corollary}

\section{Caso Fuertemente Convexo}
Consideremos primero los casos de convexidad donde sabemos que el m\'inimo existe y es \'unico, por lo tanto asumamos por ahora \ref{def: Fuertemente convexa}:

\begin{hyp}[Convexidad fuerte]
	Supongamos que la funci\'on objetivo $F : \R^d \mapsto \R$ es fuertemente convexa, es decir que cumple \ref{def: Fuertemente convexa}.
	
	Luego existe un \'unico $w_{*} \in \R^d$ tal que $F_{inf} = F(w_*) \leq F(w)$ para todo $w \in \R^d$
	
\end{hyp}

Ya estamos en condiciones de demostrar nuestro primer resultado de convergencia para \ref{algo: DE} con $\alpha_k = \alpha$, pero notemos que \textit{a priori} lo mas que podemos asumir es quedar en un entorno de $F_{inf}$ ya que de \ref{eq: Fundamental estocasticos 2 ecuacion 2} se ve que el segundo t\'ermino es constante.

Dado $w_k$ que depende de $\upxi_{1}, \dots, \upxi_{k-1}$ definamos:

\begin{equation*}
	\label{def: esperanza total}
	\expectation{F(w_k)} = \mathbb{E}_{\upxi_{1}}\mathbb{E}_{\upxi_{2}}\dots \mathbb{E}_{\upxi_{k-1}}\left[F(w_k)\right]
\end{equation*}

\begin{theorem}[Objetivo fuertemente convexo, Incremento constante]
	\label{theorem: DE en fuertemente convexo y alfa fijo converge en l1}
	Supongamos \ref{hyp: F es L lipshitz}, \ref{hyp: Acotaciones momentos de g} y \ref{def: Fuertemente convexa}; adem\'as supongamos que dado \ref{algo: DE} $\alpha_k = \alpha >0 $ constante tal que:
	
	\begin{equation}
	\label{eq: Condicion alfa Conv L1 fuertemente convexo}
	0  < \alpha \leq \dfrac{\mu}{LM_G} 
	\end{equation}
	
	Luego para todo $k \in \N$ vale que:
	
	\begin{equation*}
	\begin{array}{rcl}
	\expectation{F(w_k) - F_{inf}} & \leq & \dfrac{\alpha LM}{2 c \mu} + \left(1 - \alpha c \mu \right)^{k-1} \left(F(w_1) - F_{inf} - \dfrac{\alpha LM}{2 c \mu} \right) \\
	& \underlimitinf{k} & \dfrac{\alpha LM}{2 c \mu} 
	\end{array}
	\end{equation*}
	 
\end{theorem}

\begin{proof}
	Usando \ref{lemma: Fundamental estocasticos 2} con \ref{eq: Condicion alfa Conv L1 fuertemente convexo} y \ref{prop: Implicancias L Lipschitz} tenemos para todo $k \in \N$ que:
	
	\begin{equation*}
	\begin{array}{rcl}
		\expectationchik{F(w_{k+1}) - F(w_k)} & \underbrace{\leq}_{\ref{lemma: Fundamental estocasticos 2}} & - \left( \mu - \frac{1}{2} \alpha L M_G \right)\alpha \norm{\nabla F(w_k)}_2^2 + \frac{1}{2} \alpha^2 L M \\
		& \underbrace{\leq}_{\ref{eq: Condicion alfa Conv L1 fuertemente convexo}} & - \frac{1}{2} \alpha \mu \norm{\nabla F(w_k)}_2^2 + \frac{1}{2} \alpha ^2 LM \\
		& \underbrace{\leq}_{\ref{prop: Implicancias L Lipschitz}} & - \alpha \mu c \left(F(w_k) - F_{inf} \right)+ \frac{1}{2} \alpha ^2 LM 
	\end{array}
	\end{equation*}
	
	Luego si restamos $F_{inf}$ y tomamos esperanza total (definida en \ref{def: esperanza total}:
	
		\begin{equation*}
	\begin{array}{rcl}
	\expectationchik{F(w_{k+1}) - F(w_k)} & \leq & - \alpha \mu c \left(F(w_k) - F_{inf} \right)+ \frac{1}{2} \alpha ^2 LM  \\
	\Longrightarrow \quad \expectation{F(w_{k+1}) - F_{inf}} & \leq & \left(1 - \alpha c \mu  \right) \expectation{F(w_k) - F_{inf}} +   \frac{1}{2} \alpha ^2 LM  \\
	\Longrightarrow \quad \expectation{F(w_{k+1}) - F_{inf}} - \dfrac{\alpha L M}{2 c \mu } & \leq & \left(1 - \alpha c \mu  \right) \expectation{F(w_k) - F_{inf}} +   \frac{1}{2} \alpha ^2 LM  - \dfrac{\alpha L M}{2 c \mu } \\
	& = &  \left(1 - \alpha c \mu  \right)  \left(\expectation{F(w_k) - F_{inf}}  -  \dfrac{\alpha L M}{2 c \mu }  \right)
	\end{array}
	\end{equation*}
	
	Por otro lado notemos que:
	
	\begin{equation*}
	 0 < \alpha c \mu \leq \dfrac{c \mu^2}{L M_G} \leq \dfrac{c \mu ^2}{L \mu ^2} = \frac{c}{L} \leq 1
	\end{equation*}
	
	Luego deducimos inductivamente que:
	
	\begin{equation*}
		\expectation{F(w_{k+1}) - F_{inf}} - \dfrac{\alpha L M}{2 c \mu } \leq \left(1 - \alpha c \mu \right)^{k} \left(F(w_1) - F_{inf} - \dfrac{\alpha LM}{2 c \mu} \right)
	\end{equation*}
	\qed
\end{proof}

\begin{remark}
	Notemos que si $g$ es un estimador insesgado de $\nabla F$ entonces $\mu = M_G = 1$ por lo que $\alpha \in [0, \frac{1}{L})$ que es la condici\'on que pedimos en \ref{Dg converge a minimos}.
\end{remark}

\begin{remark}
	Notemos adem\'as que si $M=0$ (o sea el algortimo \ref{algo: DE} no tiene ruido) entonces la convergencia es lineal, recuperando el resultado de \ref{theorem: convergencia lineal GD}.
\end{remark}

\begin{remark}
	Notemos finalmente que hay un compromiso entre el primer y segundo t\'ermino de \ref{theorem: DE en fuertemente convexo y alfa fijo converge en l1} donde a un $\alpha$ m\'as cercano a $\dfrac{\mu}{LM_G}$ acelera la convergencia del primer t\'ermino, pero a costa de un entorno final de mayor vol\'umen. 
	
	Luego esto llev\'o a varios investigadores a tomar un enfoque experimental donde se tomaba un $\alpha_k = \alpha_1$ para $k \leq k_1$ donde $k_1$ es tal que $\expectation{F(w_{k_1}) - F_{inf}} \leq \dfrac{\alpha_1 LM}{2c \mu}$. Luego se tomaba $\alpha_2 = \frac{\alpha_1}{2}$ y se segu\'ia inductivamente.
\end{remark}

\begin{theorem}[Objetivo fuertemente convexo, Incremento decreciente]
	\label{theorem: DE en fuertemente convexo y alfa decreciente converge en l1}
	Supongamos \ref{hyp: F es L lipshitz}, \ref{hyp: Acotaciones momentos de g} y \ref{def: Fuertemente convexa}; adem\'as supongamos que dado \ref{algo: DE} $\alpha_k$ cumple:
	
	\begin{equation}
	\label{eq: Condicion alfa Conv L1 fuertemente convexo decreciente}
	\alpha_k =  \dfrac{\beta}{\gamma + k} \quad \ \text{ para alg\'un } \ \beta > \frac{1}{c \mu} \  \text{ y } \ \gamma > 0 \  \text{ tal que } \ \alpha_1 \leq \dfrac{\mu}{L M_G} 
	\end{equation}
	
	Luego para todo $k \in \N$ vale que:
	
	\begin{equation*}
		\expectation{F(w_k) - F_{inf}} \leq \dfrac{\eta}{\gamma + k}
	\end{equation*}
	
	Donde:
	
	\begin{equation*}
		\eta := \max \sett{\dfrac{\beta ^2 LM}{2 \left(\beta c \mu -1 \right)}, (\gamma + 1)\left(F(w_1) - F_{inf}\right)}
	\end{equation*}
	
\end{theorem}

\begin{proof}
	Notemos primero que por \ref{eq: Condicion alfa Conv L1 fuertemente convexo decreciente} para todo $k \in \N$ vale:
	
	\begin{equation*}
		\alpha_k L M_G \leq \alpha_1 L M_G \leq \mu
	\end{equation*}
	
	Luego por \ref{lemma: Fundamental estocasticos 2} y \ref{prop: Implicancias L Lipschitz} uno tiene para todo $k \in \N$:
	
	\begin{equation*}
	\begin{aligned}
		\expectationchik{F(w_{k+1})} - F(w_k) & \leq & - \left( \mu - \frac{1}{2} \alpha_k L M_G \right)\alpha_k \norm{\nabla F(w_k)}_2^2 + \frac{1}{2} \alpha_k^2 L M \\
		& \leq & - \frac{1}{2} \mu\alpha_k \norm{\nabla F(w_k)}_2^2 + \frac{1}{2} \alpha_k^2 L M \\
		& \leq & - \alpha_k c \mu \left(F(w_k) - F(w_*)\right) + \frac{1}{2} \alpha_k^2 L M
	\end{aligned}
	\end{equation*}
	
	Luego restando $F_{inf}$, tomando esperanza y reordenando vale:
	
	\begin{equation*}
		\expectation{F(w_{k+1}) - F_{inf}} \leq \left(1 - \alpha_k c \mu \right) \expectation{F(w_{k}) - F_{inf}}  + \frac{1}{2} \alpha_k^2 L M
	\end{equation*}
	
	Probemos ahora el resultado por inducci\'on. 
	
	Para el caso $k=1$ notemos que por definici\'on $\eta \geq \left(\gamma + 1\right) \left(F(w_1) - F_{inf}\right)$ por lo que trivialmente:
	
	\begin{equation*}
		\expectation{\left(F(w_1) - F_{inf}\right)} \leq \expectation{\dfrac{\eta}{\gamma +1}} = \dfrac{\eta}{\gamma + k}
	\end{equation*}
	
	Si asumimos que vale el resultado para alg\'un $k \geq 1$ entonces:
	
	\begin{equation*}
	\begin{array}{lll}
		\expectation{F(w_{k+1}) - F_{inf}} & \leq & \left(1 - \dfrac{\beta c \mu }{\gamma + k}\right) \dfrac{\eta}{\gamma + k} + \dfrac{\beta^2 LM}{2 \left(\gamma + k\right)^2} \\
		& = & \left(\dfrac{(\gamma + k) - \beta c \mu }{(\gamma + k)^2}\right) {\eta}+ \dfrac{\beta^2 LM}{2 \left(\gamma + k\right)^2} \\
		& = & \left(\dfrac{(\gamma + k) - 1}{(\gamma + k)^2}\right) {\eta}  \underbrace{- \left(\dfrac{\beta c \mu -1}{(\gamma + k)^2}\right)\eta +  \dfrac{\beta^2 LM}{2 \left(\gamma + k\right)^2}}_{\leq 0 \text{ Por definici\'on de } \eta}\\
		\underbrace{\leq}_{(\gamma + k)^2 \geq (\gamma + k + 1)(\gamma + k - 1)} & & \dfrac{\eta}{\gamma + k + 1}
	\end{array}
	\end{equation*}\qed

\end{proof}

Notemos entonces que en el caso fuertemente convexo con incrementos fijos tenemos convergencia en un entorno del m\'inimo mientras que si reducimos los incrementos tenemos convergencia en L1, cabr\'ia preguntarse (inspirados en la observaci\'on del caso $\alpha$ fijo con $M=0$) si con el ruido existente pero controlado podemos mantener la convergencia en L1.

\begin{theorem}[Objetivo Fuertemente Convexo, Reducci\'on del Ruido]
	Supongamos que valen \ref{hyp: F es L lipshitz}, \ref{hyp: Acotaciones momentos de g} y \ref{def: Fuertemente convexa} pero reforcemos \ref{eq: Hipotesis 2 Segundo momento acotado} a la existencia de una constante $M \geq 0$ y $\xi \in (0,1)$ tal que para todo $k \in \N$:
	
	\begin{equation}
	\label{eq: Varianza acotada geometricamente}
	\variancechik{g(w_k, \upxi_{k})} \leq M \xi^{k-1}
	\end{equation}
	
	Supongamos adem\'as que \ref{algo: DE} tiene $\alpha_k = \alpha$ para todo $k \in \N$ satisfaciendo:
	
	\begin{equation}
	\label{eq: Condicion alfa Conv L1 fuertemente convexo reduccion ruido}
	0  < \alpha \leq \min\sett{\dfrac{\mu}{L \mu_G^2}, \frac{1}{ \mu}} 
	\end{equation}
	
	Luego vale:
	
	\begin{equation*}
		\expectation{F(w_k) - F_{inf}} \leq \omega \rho^{k-1}
	\end{equation*}
	
	Donde:
	
	\begin{subequations}
		\begin{equation}
		\label{def: Omega en dem reduccion ruido}
		\omega := \max \sett{\dfrac{\alpha L M}{c \mu}, F(w_1) - F_{inf}}
		\end{equation}
		\begin{equation}
		\label{def: Rho en dem reduccion ruido}
		\rho := \max \sett{1 - \dfrac{\alpha c \mu }{2}, \xi} < 1
		\end{equation}
	\end{subequations}
	
\end{theorem}

\begin{proof}
	Por \ref{eq: Fundamental estocasticos 2 ecuacion 1} vale que:
	
	\begin{equation*}
		\expectationchik{F(w_{k+1})} - F(w_k) \leq  -\mu \alpha \norm{\nabla F(w_k)}_2^2 + \frac{1}{2} \alpha^2 L \expectationchik{\norm{g(w_k, \upxi_{k})}_2^2}
	\end{equation*}
	
	Luego si juntamos \ref{def: Definicion varianza condicional}, \ref{eq: Hipotesis 2 subitem 2}, \ref{eq: Condicion alfa Conv L1 fuertemente convexo reduccion ruido} y \ref{eq: Varianza acotada geometricamente} entonces:
	
	\begin{equation*}
	\begin{aligned}
		\expectationchik{F(w_{k+1})} - F(w_k) & \leq & -\mu \alpha \norm{\nabla F(w_k)}_2^2 + \frac{1}{2} \alpha^2 L \left(\mu_G^2  \norm{\nabla F(w_k)}_2^2 + M \xi^{k-1}\right) \\
		& \leq & - \left(\mu - \frac{1}{2}\alpha L \mu_G^2 \right) \alpha \norm{\nabla F(w_k)}_2^2 + \frac{1}{2} \alpha^2 LM \xi^{k-1} \\
		& \leq & -\frac{1}{2}\mu \alpha \norm{\nabla F(w_k)}_2^2 + \frac{1}{2} \alpha^2 LM \xi^{k-1} \\
		& \leq &  -c \mu \alpha \left( F(w_k) - F_{inf} \right) + \frac{1}{2} \alpha^2 LM \xi^{k-1}
	\end{aligned}
	\end{equation*}
	
	Por lo tanto:
	
	\begin{equation}
		\label{eq: Ecuacion demostracion reduccion ruido}
		\expectation{F(w_{k+1}) - F_{inf}} \leq \left(1 - c \mu \alpha\right) \expectation{F(w_k) - F_{inf}} + \frac{1}{2} \alpha^2 LM \xi^{k-1}
	\end{equation}
	
	Probemos ahora el resultado por inducci\'on. 
	
	Para el caso $k=1$ notemos que por definici\'on $\omega \geq F(w_1) - F_{inf}$ por lo que:
	
	\begin{equation*}
		\expectation{\F(w_1) - F_{inf}} \leq \expectation{\omega} = \omega = \omega \rho^{0} = \omega \rho^{k-1}
	\end{equation*}
	
	Si asumimos que vale para alg\'un $k \geq 1$ entonces de \ref{eq: Ecuacion demostracion reduccion ruido}, \ref{def: Omega en dem reduccion ruido} y \ref{def: Rho en dem reduccion ruido}:
	
	\begin{equation*}
	\begin{aligned}
	\expectation{F(w_{k+1}) - F_{inf}} & \leq &  \left(1 - c \mu \alpha\right) \omega \rho^{k-1}+ \frac{1}{2} \alpha^2 LM \xi^{k-1} \\
	& = & \omega \rho^{k-1} \left(1 - c \mu \alpha + \dfrac{\alpha^2 L M}{2 \omega} \left(\frac{\xi}{\rho}\right)^{k-1} \right) \\
	& \leq &  \omega \rho^{k-1} \left(1 - c \mu \alpha + \dfrac{\alpha^2 L M}{2 \omega} \right) \\
	& \leq &  \omega \rho^{k-1} \left(1 - c \mu \alpha + \dfrac{\alpha c \mu}{2} \right) \\
	&  = &  \omega \rho^{k-1} \left(1 - \dfrac{\alpha c \mu}{2} \right) \\
	& \leq & \omega \rho^{k}
	\end{aligned}
	\end{equation*}
	\qed
\end{proof}

\section{Caso general}

Manteniendo las mismas hip\'otesis y notaciones veamos el caso general, nuevamente separando entre incrementos constantes o decrecientes.

\begin{theorem}[Objetivo no convexo, Incrementos fijos]
	Asumiendo \ref{hyp: F es L lipshitz} y \ref{hyp: Acotaciones momentos de g} y suponiendo que en \ref{algo: DE} tenemos $\alpha_k = \alpha$ tal que:
	
	\begin{equation}
	\label{eq: Condicion alfa Conv L1 no convexo}
	0  < \alpha \leq \dfrac{\mu}{LM_G} 
	\end{equation}
	
	Entonces vale para todo $k \in \N$:
	
	\begin{subequations}
		\begin{equation}
		\label{eq: Objetivo no convexo, incremento fijo, 1}
		\expectation{\sum\limits_{k=1}^{K} {\norm{\nabla F (w)_k)}_2^2}} \leq \dfrac{K \alpha LM}{\mu} + \dfrac{2(F(w_1) - F_{inf})}{\mu \alpha}
		\end{equation}
		\begin{equation}
		\label{eq: Objetivo no convexo, incremento fijo, 2}
		\begin{array}{rcl}
		\expectation{\dfrac{1}{K}\sum\limits_{k=1}^{K} {\norm{\nabla F (w)_k)}_2^2}} & \leq & \dfrac{\alpha LM}{\mu} + \dfrac{2(F(w_1) - F_{inf})}{K \mu \alpha} \\
		& \underlimitinf{K} &  \dfrac{\alpha LM}{\mu}
		\end{array}
		\end{equation}
	\end{subequations}
	
\end{theorem}

\begin{proof}
	Recordemos \ref{eq: Fundamental estocasticos 2 ecuacion 1} y si tomamos esperanza total e imponemos \ref{eq: Condicion alfa Conv L1 no convexo} tenemos:
	
	\begin{equation*}
	\begin{array}{rcl}
	\expectation{F(w_{k+1})} - \expectation{F(w_k)} & \leq & - \left(\mu - \frac{1}{2}\alpha L M_G \right) \alpha \expectation{\norm{\nabla F(w_k)}_2^2} + \frac{1}{2} \alpha^2 LM \\
	& \leq & - \frac{1}{2}\alpha \mu \expectation{\norm{\nabla F(w_k)}_2^2} + \frac{1}{2} \alpha^2 LM
	\end{array}
	\end{equation*}
	
	Luego como por \ref{hyp: Acotaciones momentos de g} tenemos que $F_{inf} \leq \expectation{F(w_k)}$ para todo $k \in \N$ vale:
	
	\begin{equation*}
		F_{inf} - F(w_1) \leq \expectation{F(w_{K+1})} - F(w_1) \leq  - \frac{1}{2}\alpha \mu \sum\limits_{1}^{K}{\expectation{\norm{\nabla F(w_k)}_2^2}} + \frac{1}{2} K \alpha^2 LM
	\end{equation*}
	\qed
\end{proof}

\begin{remark}
	Notemos que si $M=0$ (no hay ruido o crece comparable a $\norm{\nabla F(w_k)}^2_2$) entonces obtenemos que $\sum\limits_{k=1}^{\infty} {\norm{\nabla F (w)_k)}_2^2} < \infty$ por lo que $\sett{\norm{\nabla F (w_k)}_2^2}_{k \in \N} \underlimitinf{K} 0 $, que es el resultado obtenido en \cite{nesterov:2004}.
	
	En cambio, cuando $M \neq 0$ aunque no podemos acotar $\norm{\nabla F (w_k)}_2^2$ \textit{per-se}, podemos decir de \ref{eq: Objetivo no convexo, incremento fijo, 2} que en esperanza el valor del gradiente es cada vez menor en un entorno de radio $\dfrac{\alpha LM}{\mu}$. Luego recuperamos la intuici\'on del caso convexo (\ref{theorem: DE en fuertemente convexo y alfa fijo converge en l1}) donde a menor incremento el entorno es menor (el algortimo es m\'as preciso) pero la cantidad de iteraciones es mayor.
\end{remark}

Para el de incrementos decrecientes, asumamos que $\sett{\alpha_k}$ cumple la condici\'on de \textit{Robbins - Monro} \ref{eq: Condicion robbins monro}:

\begin{theorem}[Objetivo no convexo, Incrementos decrecientes]
	\label{theorem: Convergencia L1, Objetivo no convexo, Incrementos decrecientes}
	Asumiendo \ref{hyp: F es L lipshitz} y \ref{hyp: Acotaciones momentos de g}, suponiendo adem\'as que en \ref{algo: DE} los $\sett{\alpha_k}$ satisfacen \ref{eq: Condicion robbins monro}; si notamos $A_K := \sum\limits_{k=1}^{K} {\alpha_k}$ vale para todo $k \in \N$::
	
	\begin{subequations}
		\begin{equation}
		\label{eq: Objetivo no convexo, incremento decreciente, 1}
		\lim\limits_{K \rightarrow \infty} {\expectation{\sum\limits_{k=1}^{K} {\alpha_k \norm{\nabla F (w)_k)}_2^2}}} < \infty
		\end{equation}
		\begin{equation}
		\label{eq: Objetivo no convexo, incremento decreciente, 2}
		\expectation{\dfrac{1}{A_K}\sum\limits_{k=1}^{K} {\alpha_k \norm{\nabla F (w)_k)}_2^2}} \underlimitinf{K}  0
		\end{equation}
	\end{subequations}
	
\end{theorem}

\begin{proof}
	Como $\alpha_k \rightarrow 0$ por \ref{eq: Condicion robbins monro} entonces podemos asumir sin p\'erdida de generalidad que $\alpha_k L M_G \leq \mu$ para todo $k \in \N$, luego:
	
		\begin{equation*}
	\begin{array}{rcl}
	\expectation{F(w_{k+1})} - \expectation{F(w_k)} & \leq & - \left(\mu - \frac{1}{2}\alpha_k L M_G \right) \alpha_k \expectation{\norm{\nabla F(w_k)}_2^2} + \frac{1}{2} \alpha_k^2 LM \\
	& \leq & - \frac{1}{2}\alpha_k \mu \expectation{\norm{\nabla F(w_k)}_2^2} + \frac{1}{2} \alpha_k^2 LM
	\end{array}
	\end{equation*}
	
	Luego como por \ref{hyp: Acotaciones momentos de g} tenemos que $F_{inf} \leq \expectation{F(w_k)}$ para todo $k \in \N$ vale:
	
	\begin{equation*}
	F_{inf} - \expectation{F(w_1)} \leq \expectation{F(w_{K+1})} - \expectation{F(w_1)} \leq  - \frac{1}{2}\mu \sum\limits_{1}^{K}{\alpha_k \expectation{\norm{\nabla F(w_k)}_2^2}} + \frac{1}{2} LM \sum\limits_{k=1}^{K} {\alpha_k^2}
	\end{equation*}
	
	Luego:
	
		\begin{equation*}
	\sum\limits_{1}^{K}{\alpha_k \expectation{\norm{\nabla F(w_k)}_2^2}}  \leq  \dfrac{2 (\expectation{F(w_1) - F_{inf} })}{\mu} + \frac{LM}{\mu}  \underbrace{\sum\limits_{k=1}^{K} {\alpha_k^2}}_{\underrightarrow{K \rightarrow \infty} \ C  \ < \ \infty }
	\end{equation*}
	
	Por lo que \ref{eq: Objetivo no convexo, incremento decreciente, 1} esta probado. Finalmente como por \ref{eq: Condicion robbins monro} tenemos que $A_K \rightarrow \infty$ podemos ver que existe $D>0$ constante tal que:
	\begin{equation*}
	\expectation{\dfrac{1}{A_K}\sum\limits_{k=1}^{K} {\alpha_k \norm{\nabla F (w)_k)}_2^2}} \leq \dfrac{D}{A_K} \underlimitinf{K} 0
	\end{equation*}
	se tiene \ref{eq: Objetivo no convexo, incremento decreciente, 2}.\qed
	
\end{proof}

\begin{corollary}
	\label{coro: Gradientes cerca de cero, Objetivo no convexo, Incrementos decrecientes}
	Asumiendo \ref{hyp: F es L lipshitz} y \ref{hyp: Acotaciones momentos de g}, suponiendo adem\'as que en \ref{algo: DE} los $\sett{\alpha_k}$ satisfacen \ref{eq: Condicion robbins monro} entonces : 
	
	\begin{equation}
		\liminf\limits_{k \rightarrow \infty} {\expectation{\norm{\nabla F (w_k)}_2^2}} = 0 
	\end{equation}
	
\end{corollary}

\begin{corollary}
	Bajo las mismas hip\'otesis de \ref{theorem: Convergencia L1, Objetivo no convexo, Incrementos decrecientes} sea $k(K) \in \sett{1, \dots, K}$ un \'indice aleatorio elegido con probabilidades respectivas $\sett{\alpha_k}_{k=1}^K$; luego $\norm{\nabla F(w_k)}_2 \rightarrow 0$ en probabilidad.
\end{corollary}

\begin{proof}
	Sea $\epsilon > 0$, luego de $\ref{eq: Objetivo no convexo, incremento decreciente, 1}$ y la desigualdad de Markov:
	
	\begin{equation*}
		\mathbb{P} \left[\norm{\nabla F (w_k)}_2 \geq \epsilon\right] = \mathbb{P} \left[\norm{\nabla F (w_k)}_2^2 \geq \epsilon^2\right] \leq \epsilon^{-2} \expectation{\norm{\nabla F(w_k)}_2^2}  \rightarrow 0
	\end{equation*}
\qed
	
\end{proof}.

\begin{theorem}[Objetivo no convexo regular, Incrementos decrecientes]
	Bajo las mismas hip\'otesis de \ref{theorem: Convergencia L1, Objetivo no convexo, Incrementos decrecientes} si adem\'as pedimos que $F \in C^2$ y que $w \mapsto \norm{\nabla F (w)}_2^2$ sea $l-$ Lipshitz entonces:
	
	\begin{equation}
		\lim\limits_{k \rightarrow \infty} \expectation{\norm{\nabla F (w_k)}_2^2} = 0
	\end{equation}
	
\end{theorem}

\begin{proof}
	Sea $G(w) := \norm{\nabla F(w)}_2^2$ y sea $L_G$ la constante de Lipshitz de $\nabla G(w) = 2 \nabla^2 F(w) \nabla F(w)$, luego:
	
	\begin{equation*}
	\begin{aligned}
		G(w_{k+1}) - G(w_k) & \underbrace{\leq}_{\ref{obs: F es l lipshitz}} & \nabla G(w_k)^T (w_{k+1} - w_k) + \frac{1}{2} L_G \norm{w_k - w_{k+1}}_2^2 \\
		& \leq & - \alpha_k \nabla G(w_k)^T g(w_k, \upxi_{k}) + \frac{1}{2} \alpha_k L_G  \norm{g(w_k, \upxi_{k})}_2^2
	\end{aligned}
	\end{equation*}
	
	Si tomamos esperanza condicional a $\upxi_{k}$ y usamos \ref{hyp: F es L lipshitz}, \ref{hyp: Acotaciones momentos de g} entonces:
	
	\begin{equation*}
	\begin{aligned}
	\expectationchik{G(w_{k+1}) - G(w_k)} & \leq & -2 \alpha_k \nabla F(w_k)^T \nabla^2 F(w_k)^T \expectationchik{g(w_k, \upxi_{k})} + \\
	&& \frac{1}{2} \alpha_k^2 L_G \expectationchik{\norm{g(w_k, \upxi_{k})}^2_2} \\
	& \leq & 2 \alpha_k \norm{\nabla F(w_k)}_2 \norm{\nabla^2 F(w_k)}_2 \norm{\expectationchik{g(w_k, \upxi_{k})}}_2 +  \\
	&& \frac{1}{2} \alpha_k^2 L_G \expectationchik{\norm{g(w_k, \upxi_{k})}^2_2} \\
	& \leq & 2 \alpha_k L \mu_G \norm{\nabla F (w_k)}_2^2 + \\
	 && \frac{1}{2} \alpha_k^2 L_G\left(M + M_V \norm{\nabla F(w_k)}_2^2\right)
	\end{aligned}
	\end{equation*}
	
	Luego obtenemos tomando esperanza total:
	
	\begin{equation}
	\label{eq: Objetivo no convexo regular, Incrementos decrecientes}
	\expectation{G(w_{k+1})} - \expectation{G(w_k)} \leq 2 \alpha_k L \mu_G \expectation{\norm{\nabla F (w_k)}_2^2} +  \frac{1}{2} \alpha_k^2 L_G\left(M + M_V \expectation{\norm{\nabla F(w_k)}_2^2}\right)
	\end{equation}
	
	Notemos que existe $K\in \N$ tal que $\alpha_k^2 \leq \alpha_k$ y luego por \ref{theorem: Convergencia L1, Objetivo no convexo, Incrementos decrecientes} el lado derecho cumple:
	
	\begin{equation*}
	\lim\limits_{N \rightarrow \infty} { {2 L \mu_G \underbrace{\sum\limits_{k=K}^{K+N}\expectation{\alpha_k\norm{\nabla F (w_k)}_2^2}}_{\ref{eq: Objetivo no convexo, incremento decreciente, 1}} +  \frac{1}{2} L_G\left(M\underbrace{\sum\limits_{k=K}^{K+N}\alpha_k^2}_{\ref{eq: Condicion robbins monro}} + M_V \underbrace{\sum\limits_{k=K}^{K+N}\expectation{\alpha_k^2\norm{\nabla F(w_k)}_2^2}}_{{\ref{eq: Objetivo no convexo, incremento decreciente, 1}}}\right)} } = 0
	\end{equation*}
	
	Sean:
	
	\begin{equation*}
	\begin{aligned}
	S_K^+ & = & \sum\limits_{k=1}^{K} {\max\left(0, \expectation{G(w_{k+1})} - \expectation{G(w_k)} \right)} \\
	S_K^- & = & \sum\limits_{k=1}^{K} {\max\left(0, \expectation{G(w_{k})} - \expectation{G(w_{k+1})} \right)}
	\end{aligned}
	\end{equation*}
	
	Luego como en \ref{eq: Objetivo no convexo regular, Incrementos decrecientes} el lado derecho es positivo y su suma es convergente tenemos que $\sett{S_K^+}$ es mon\'otona, acotada superiormente y por ende convergente. Adem\'as como $0 \leq \expectation{G(w_k)} = \expectation{G(w_0)} + S_k^+ - S_k^-$ tenemos que $\sett{S_K^-}$ tambi\'en es mon\'otona y acotada superiormente, por lo que es convergente; conclu\'imos que $\expectation{G(w_k)}$ debe ser convergente, y por \ref{coro: Gradientes cerca de cero, Objetivo no convexo, Incrementos decrecientes} tenemos $\expectation{\norm{\nabla F(w_k)}_2^2} = \expectation{G(w_k)} \rightarrow 0$. \qed	
\end{proof}

