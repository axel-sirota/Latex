\chapter{Convergencia en L1}\label{ch:convergenciaL1}

\section{Caso Fuertemente Convexo}
Consideremos primero los casos de convexidad donde sabemos que el m\'inimo existe y es \'unico, por lo tanto asumamos por ahora:

\begin{hyp}[Convexidad fuerte]
	\label{hyp: Convexidad fuerte}
	Supongamos que la funci\'on objetivo $F : \R^d \mapsto \R$ cumple que existe $c >0$ tal que para todos $z,w \in \R^d$:
	
	\begin{equation}
		F(z) \geq F(w) + \nabla F(w)^T (z-w) + \frac{1}{2} c \norm{z-w}_2^2
	\end{equation}
	
	Luego existe un \'unico $w_{*} \in \R^d$ tal que $F_{inf} = F(w_*) \leq F(w)$ para todo $w \in \R^d$
	
\end{hyp}

Notemos que de \ref{hyp: Convexidad fuerte} y \ref{obs: F es l lipshitz} vale que $c \leq L$ 

\begin{lemma}
	\label{eq: Desigualdad convexidad fuerte}
	Supongamos que $F$ cumple \ref{hyp: Convexidad fuerte}, luego para todo $w \in \R^d$ vale que:
	
	\begin{equation}
		2c\left(F(w) - F_{inf}\right) \leq \norm{\nabla F(w)}_2^2
	\end{equation}
	
\end{lemma}

\begin{proof}
	Dado $w \in \R^d$ sea:
	
	\begin{equation*}
		q(z) = F(w) + \nabla F(w)^T (z-w) + \frac{1}{2} c \norm{z-w}_2^2
	\end{equation*}
	
	Se puede verificar que $z_* := w - \frac{1}{c} \nabla F(w)$ cumple que $q(z_*) = F(w) - \frac{1}{2c} \norm{\nabla F(w)}_2^2 \leq q(z)$ para todo $z \in \R^d$; luego por \ref{eq: Desigualdad convexidad fuerte} se tiene:
	
	\begin{equation*}
		F_{inf} \geq F(w) + \nabla F(w)^T (w_*-w) + \frac{1}{2} c \norm{w_*-w}_2^2 \geq F(w) - \frac{1}{2c} \norm{\nabla F(w)}_2^2 
	\end{equation*}
	\qed
\end{proof}

Ya estamos en condiciones de demostrar nuestro primer resultado de convergencia para \ref{algo: DE} con $\alpha_k = \alpha$, pero notemos que \textit{a priori} lo mas que podemos asumir es quedar en un entorno de $F_{inf}$ ya que de \ref{eq: Fundamental estocasticos 2 ecuacion 2} se ve que el segundo t\'ermino es constante.

Dado $w_k$ que depende de $\upxi_{1}, \dots, \upxi_{k-1}$ definamos:

\begin{equation*}
	\label{def: esperanza total}
	\expectation{F(w_k)} = \mathbb{E}_{\upxi_{1}}\mathbb{E}_{\upxi_{2}}\dots \mathbb{E}_{\upxi_{k-1}}\left[F(w_k)\right]
\end{equation*}

\begin{theorem}[Objetivo fuertemente convexo, Incremento constante]
	\label{theorem: DE en fuertemente convexo y alfa fijo converge en l1}
	Supongamos \ref{hyp: F es L lipshitz}, \ref{hyp: Acotaciones momentos de g} y \ref{hyp: Convexidad fuerte}; adem\'as supongamos que dado \ref{algo: DE} $\alpha_k = \alpha >0 $ constante tal que:
	
	\begin{equation}
	\label{eq: Condicion alfa Conv L1 fuertemente convexo}
	0  < \alpha \leq \dfrac{\mu}{LM_G} 
	\end{equation}
	
	Luego para todo $k \in \N$ vale que:
	
	\begin{equation*}
	\begin{array}{rcl}
	\expectation{F(w_k) - F_{inf}} & \leq & \dfrac{\alpha LM}{2 c \mu} + \left(1 - \alpha c \mu \right)^{k-1} \left(F(w_1) - F_{inf} - \dfrac{\alpha LM}{2 c \mu} \right) \\
	& \underrightarrow{k \rightarrow \infty} & \dfrac{\alpha LM}{2 c \mu} 
	\end{array}
	\end{equation*}
	 
\end{theorem}

\begin{proof}
	Usando \ref{lemma: Fundamental estocasticos 2} con \ref{eq: Condicion alfa Conv L1 fuertemente convexo} y \ref{eq: Desigualdad convexidad fuerte} tenemos para todo $k \in \N$ que:
	
	\begin{equation*}
	\begin{array}{rcl}
		\expectationchik{F(w_{k+1}) - F(w_k)} & \underbrace{\leq}_{\ref{lemma: Fundamental estocasticos 2}} & - \left( \mu - \frac{1}{2} \alpha L M_G \right)\alpha \norm{\nabla F(w_k)}_2^2 + \frac{1}{2} \alpha^2 L M \\
		& \underbrace{\leq}_{\ref{eq: Condicion alfa Conv L1 fuertemente convexo}} & - \frac{1}{2} \alpha \mu \norm{\nabla F(w_k)}_2^2 + \frac{1}{2} \alpha ^2 LM \\
		& \underbrace{\leq}_{\ref{eq: Desigualdad convexidad fuerte}} & - \alpha \mu c \left(F(w_k) - F_{inf} \right)+ \frac{1}{2} \alpha ^2 LM 
	\end{array}
	\end{equation*}
	
	Luego si restamos $F_{inf}$ y tomamos esperanza total (definida en \ref{def: esperanza total}:
	
		\begin{equation*}
	\begin{array}{rcl}
	\expectationchik{F(w_{k+1}) - F(w_k)} & \leq & - \alpha \mu c \left(F(w_k) - F_{inf} \right)+ \frac{1}{2} \alpha ^2 LM  \\
	\Longrightarrow \quad \expectation{F(w_{k+1}) - F_{inf}} & \leq & \left(1 - \alpha c \mu  \right) \expectation{F(w_k) - F_{inf}} +   \frac{1}{2} \alpha ^2 LM  \\
	\Longrightarrow \quad \expectation{F(w_{k+1}) - F_{inf}} - \dfrac{\alpha L M}{2 c \mu } & \leq & \left(1 - \alpha c \mu  \right) \expectation{F(w_k) - F_{inf}} +   \frac{1}{2} \alpha ^2 LM  - \dfrac{\alpha L M}{2 c \mu } \\
	& = &  \left(1 - \alpha c \mu  \right)  \left(\expectation{F(w_k) - F_{inf}}  -  \dfrac{\alpha L M}{2 c \mu }  \right)
	\end{array}
	\end{equation*}
	
	Por otro lado notemos que:
	
	\begin{equation*}
	 0 < \alpha c \mu \leq \dfrac{c \mu^2}{L M_G} \leq \dfrac{c \mu ^2}{L \mu ^2} = \frac{c}{L} \leq 1
	\end{equation*}
	
	Luego deducimos inductivamente que:
	
	\begin{equation*}
		\expectation{F(w_{k+1}) - F_{inf}} - \dfrac{\alpha L M}{2 c \mu } \leq \left(1 - \alpha c \mu \right)^{k} \left(F(w_1) - F_{inf} - \dfrac{\alpha LM}{2 c \mu} \right)
	\end{equation*}
	\qed
\end{proof}

\begin{remark}
	Notemos que si $g$ es un estimador insesgado de $\nabla F$ entonces $\mu = M_G = 1$ por lo que $\alpha \in [0, \frac{1}{L})$ que es la condici\'on que pedimos en \ref{Dg converge a minimos}.
\end{remark}

\begin{remark}
	Notemos adem\'as que si $M=0$ (o sea el algortimo \ref{algo: DE} no tiene ruido) entonces la convergencia es lineal, recuperando el resultado de \cite{nesterov:2004}.
\end{remark}

\begin{remark}
	Notemos finalmente que hay un compromiso entre el primer y segundo t\'ermino de \ref{theorem: DE en fuertemente convexo y alfa fijo converge en l1} donde a un $\alpha$ m\'as cercano a $\dfrac{\mu}{LM_G}$ acelera la convergencia del primer t\'ermino, pero a costa de un entorno final de mayor vol\'umen. 
	
	Luego esto llevo a varios investigadores a tomar un enfoque artesanal donde se tomaba un $\alpha_k = \alpha_1$ para $k \leq k_1$ donde $k_1$ es tal que $\expectation{F(w_{k_1}) - F_{inf}} \leq \dfrac{\alpha_1 LM}{2c \mu}$. Luego se tomaba $\alpha_2 = \frac{\alpha_1}{2}$ y se segu\'ia inductivamente.
\end{remark}

\begin{theorem}[Objetivo fuertemente convexo, Incremento decreciente]
	\label{theorem: DE en fuertemente convexo y alfa decreciente converge en l1}
	Supongamos \ref{hyp: F es L lipshitz}, \ref{hyp: Acotaciones momentos de g} y \ref{hyp: Convexidad fuerte}; adem\'as supongamos que dado \ref{algo: DE} $\alpha_k$ cunmple:
	
	\begin{equation}
	\label{eq: Condicion alfa Conv L1 fuertemente convexo decreciente}
	\alpha_k =  \dfrac{\beta}{\gamma + k} \quad \ \text{ para alg\'un } \ \beta > \frac{1}{c \mu} \  \text{ y } \ \gamma > 0 \  \text{ tal que } \ \alpha_1 \leq \dfrac{\mu}{L M_G} 
	\end{equation}
	
	Luego para todo $k \in \N$ vale que:
	
	\begin{equation*}
		\expectation{F(w_k) - F_{inf}} \leq \dfrac{\eta}{\gamma + k}
	\end{equation*}
	
	Donde:
	
	\begin{equation*}
		\eta := \max \sett{\dfrac{\beta ^2 LM}{2 \left(\beta c \mu -1 \right)}, (\gamma + 1)\left(F(w_1) - F_{inf}\right)}
	\end{equation*}
	
\end{theorem}

\begin{proof}
	Notemos primero que por \ref{eq: Condicion alfa Conv L1 fuertemente convexo decreciente} para todo $k \in \N$ vale:
	
	\begin{equation*}
		\alpha_k L M_G \leq \alpha_1 L M_G \leq \mu
	\end{equation*}
	
	Luego por \ref{lemma: Fundamental estocasticos 2} y \ref{eq: Desigualdad convexidad fuerte} uno tiene para todo $k \in \N$:
	
	\begin{equation*}
	\begin{aligned}
		\expectationchik{F(w_{k+1})} - F(w_k) & \leq & - \left( \mu - \frac{1}{2} \alpha_k L M_G \right)\alpha_k \norm{\nabla F(w_k)}_2^2 + \frac{1}{2} \alpha_k^2 L M \\
		& \leq & - \frac{1}{2} \mu\alpha_k \norm{\nabla F(w_k)}_2^2 + \frac{1}{2} \alpha_k^2 L M \\
		& \leq & - \alpha_k c \mu \left(F(w_k) - F(w_*)\right) + \frac{1}{2} \alpha_k^2 L M
	\end{aligned}
	\end{equation*}
	
	Luego restando $F_{inf}$, tomando esperanza y reordenando vale:
	
	\begin{equation*}
		\expectation{F(w_{k+1}) - F_{inf}} \leq \left(1 - \alpha_k c \mu \right) \expectation{F(w_{k}) - F_{inf}}  + \frac{1}{2} \alpha_k^2 L M
	\end{equation*}
	
	Probemos ahora el resultado por inducci\'on. Por la definici\'on de $\eta$ tenemos que $k = 1$ vale, luego si asumimos que vale el resultado para alg\'un $k \geq 1$ entonces:
	
		\marginpar{Porque vale k=1?}
	
	\begin{equation*}
	\begin{array}{lll}
		\expectation{F(w_{k+1}) - F_{inf}} & \leq & \left(1 - \dfrac{\beta c \mu }{\gamma + k}\right) \dfrac{\eta}{\gamma + k} + \dfrac{\beta^2 LM}{2 \left(\gamma + k\right)^2} \\
		& = & \left(\dfrac{(\gamma + k) - \beta c \mu }{(\gamma + k)^2}\right) {\eta}+ \dfrac{\beta^2 LM}{2 \left(\gamma + k\right)^2} \\
		& = & \left(\dfrac{(\gamma + k) - 1}{(\gamma + k)^2}\right) {\eta}  \underbrace{- \left(\dfrac{\beta c \mu -1}{(\gamma + k)^2}\right)\eta +  \dfrac{\beta^2 LM}{2 \left(\gamma + k\right)^2}}_{\leq 0 \text{ Por definici\'on de } \eta}\\
		\underbrace{\leq}_{(\gamma + k)^2 \geq (\gamma + k + 1)(\gamma + k - 1)} & & \dfrac{\eta}{\gamma + k + 1}
	\end{array}
	\end{equation*}\qed

\end{proof}

Notemos entonces que en el caso fuertemente convexo con incrementos fijos tenemos convergencia en un entorno del m\'inimo mientras que si reducimos los incrementos tenemos convergencia en L1, cabr\'ia preguntarse (inspirados en la observaci\'on del caso $\alpha$ fijo con $M=0$) si con el ruido existente pero controlado podemos mantener la convergencia en L1.

\begin{theorem}[Objetivo Fuertemente Convexo, Reducci\'on del Ruido]
	Supongamos que valen \ref{hyp: F es L lipshitz}, \ref{hyp: Acotaciones momentos de g} y \ref{hyp: Convexidad fuerte} pero reforcemos \ref{eq: Hipotesis 2 Segundo momento acotado} a la existencia de una constante $M \geq 0$ y $\xi \in (0,1)$ tal que para todo $k \in \N$:
	
	\begin{equation}
	\label{eq: Varianza acotada geometricamente}
	\variancechik{g(w_k, \upxi_{k})} \leq M \xi^{k-1}
	\end{equation}
	
	Supongamos adem\'as que \ref{algo: DE} tiene $\alpha_k = \alpha$ para todo $k \in \N$ satisfaciendo:
	
	\begin{equation}
	\label{eq: Condicion alfa Conv L1 fuertemente convexo reduccion ruido}
	0  < \alpha \leq \min\sett{\dfrac{\mu}{L \mu_G^2}, \frac{1}{ \mu}} 
	\end{equation}
	
	Luego vale:
	
	\begin{equation*}
		\expectation{F(w_k) - F_{inf}} \leq \omega \rho^{k-1}
	\end{equation*}
	
	Donde:
	
	\begin{subequations}
		\begin{equation}
		\label{def: Omega en dem reduccion ruido}
		\omega := \max \sett{\dfrac{\alpha L M}{c \mu}, F(w_1) - F_{inf}}
		\end{equation}
		\begin{equation}
		\label{def: Rho en dem reduccion ruido}
		\rho := \max \sett{1 - \dfrac{\alpha c \mu }{2}, \xi} < 1
		\end{equation}
	\end{subequations}
	
\end{theorem}

\section{Caso general}

Manteniendo las mismas hip\'otesis y notaciones veamos el caso general, nuevamente separando entre incrementos constantes o decrecientes.

\begin{theorem}[Objetivo no convexo, Incrementos fijos]
	Asumiendo \ref{hyp: F es L lipshitz} y \ref{hyp: Acotaciones momentos de g} y suponiendo que en \ref{algo: DE} tenemos $\alpha_k = \alpha$ tal que:
	
	\begin{equation}
	\label{eq: Condicion alfa Conv L1 no convexo}
	0  < \alpha \leq \dfrac{\mu}{LM_G} 
	\end{equation}
	
	Entonces vale para todo $k \in \N$:
	
	\begin{subequations}
		\begin{equation}
		\label{eq: Objetivo no convexo, incremento fijo, 1}
		\expectation{\sum\limits_{k=1}^{K} {\norm{\nabla F (w)_k)}_2^2}} \leq \dfrac{K \alpha LM}{\mu} + \dfrac{2(F(w_1) - F_{inf})}{\mu \alpha}
		\end{equation}
		\begin{equation}
		\label{eq: Objetivo no convexo, incremento fijo, 2}
		\begin{array}{rcl}
		\expectation{\dfrac{1}{K}\sum\limits_{k=1}^{K} {\norm{\nabla F (w)_k)}_2^2}} & \leq & \dfrac{\alpha LM}{\mu} + \dfrac{2(F(w_1) - F_{inf})}{K \mu \alpha} \\
		& \underrightarrow{K \rightarrow \infty} &  \dfrac{\alpha LM}{\mu}
		\end{array}
		\end{equation}
	\end{subequations}
	
\end{theorem}

\begin{proof}
	Recordemos \ref{eq: Fundamental estocasticos 2 ecuacion 1} y si tomamos esperanza total e imponemos \ref{eq: Condicion alfa Conv L1 no convexo} tenemos:
	
	\begin{equation*}
	\begin{array}{rcl}
	\expectation{F(w_{k+1})} - \expectation{F(w_k)} & \leq & - \left(\mu - \frac{1}{2}\alpha L M_G \right) \alpha \expectation{\norm{\nabla F(w_k)}_2^2} + \frac{1}{2} \alpha^2 LM \\
	& \leq & - \frac{1}{2}\alpha \mu \expectation{\norm{\nabla F(w_k)}_2^2} + \frac{1}{2} \alpha^2 LM
	\end{array}
	\end{equation*}
	
	Luego como por \ref{hyp: Acotaciones momentos de g} tenemos que $F_{inf} \leq \expectation{F(w_k)}$ para todo $k \in \N$ vale:
	
	\begin{equation*}
		F_{inf} - F(w_1) \leq \expectation{F(w_{K+1})} - F(w_1) \leq  - \frac{1}{2}\alpha \mu \sum\limits_{1}^{K}{\expectation{\norm{\nabla F(w_k)}_2^2}} + \frac{1}{2} K \alpha^2 LM
	\end{equation*}
	\qed
\end{proof}

\begin{remark}
	Notemos que si $M=0$ (no hay ruido o crece comparable a $\norm{\nabla F(w_k)}^2_2$) entonces obtenemos que $\sum\limits_{k=1}^{\infty} {\norm{\nabla F (w)_k)}_2^2} < \infty$ por lo que $\sett{\norm{\nabla F (w_k)}_2^2}_{k \in \N} \underrightarrow{k \rightarrow \infty} 0 $, que es el resultado obtenido en \cite{nesterov:2004}.
	
	En cambio, cuando $M \neq 0$ aunque no podemos acotar $\norm{\nabla F (w_k)}_2^2$ \textit{per-se}, podemos decir de \ref{eq: Objetivo no convexo, incremento fijo, 2} que en esperanza el valor del gradiente es cada vez menor en un entorno de radio $\dfrac{\alpha LM}{\mu}$. Luego recuperamos la intuici\'on del caso convexo (\ref{theorem: DE en fuertemente convexo y alfa fijo converge en l1}) donde a menor incremento el entorno es menor (el algortimo es m\'as preciso) pero la cantidad de iteraciones es mayor.
\end{remark}

Para el de incrementos decrecientes, asumamos que $\sett{\alpha_k}$ cumple la condici\'on de \textit{Robbins - Monro} (ver \cite{robbins:1951}):

\begin{equation}
\label{eq: Condicion robbins monro}
\sum\limits_{k=1}^{\infty} {\alpha_k} = \infty \quad \text{ y } \quad \sum\limits_{k=1}^{\infty} {\alpha_k^2} < \infty
\end{equation}

\begin{theorem}[Objetivo no convexo, Incrementos decrecientes]
	\label{theorem: Convergencia L1, Objetivo no convexo, Incrementos decrecientes}
	Asumiendo \ref{hyp: F es L lipshitz} y \ref{hyp: Acotaciones momentos de g}, suponiendo adem\'as que en \ref{algo: DE} los $\sett{\alpha_k}$ satisfacen \ref{eq: Condicion robbins monro}; si notamos $A_K := \sum\limits_{k=1}^{K} {\alpha_k}$ vale para todo $k \in \N$::
	
	\begin{subequations}
		\begin{equation}
		\label{eq: Objetivo no convexo, incremento decreciente, 1}
		\lim\limits_{K \rightarrow \infty} {\expectation{\sum\limits_{k=1}^{K} {\alpha_k \norm{\nabla F (w)_k)}_2^2}}} < \infty
		\end{equation}
		\begin{equation}
		\label{eq: Objetivo no convexo, incremento decreciente, 2}
		\expectation{\dfrac{1}{A_K}\sum\limits_{k=1}^{K} {\alpha_k \norm{\nabla F (w)_k)}_2^2}} \underrightarrow{K \rightarrow \infty} \ 0
		\end{equation}
	\end{subequations}
	
\end{theorem}

\begin{proof}
	Como $\alpha_k \rightarrow 0$ por \ref{eq: Condicion robbins monro} entonces podemos asumir sin p\'erdida de generalidad que $\alpha_k L M_G \leq \mu$ para todo $k \in \N$, luego:
	
		\begin{equation*}
	\begin{array}{rcl}
	\expectation{F(w_{k+1})} - \expectation{F(w_k)} & \leq & - \left(\mu - \frac{1}{2}\alpha_k L M_G \right) \alpha_k \expectation{\norm{\nabla F(w_k)}_2^2} + \frac{1}{2} \alpha_k^2 LM \\
	& \leq & - \frac{1}{2}\alpha_k \mu \expectation{\norm{\nabla F(w_k)}_2^2} + \frac{1}{2} \alpha_k^2 LM
	\end{array}
	\end{equation*}
	
	Luego como por \ref{hyp: Acotaciones momentos de g} tenemos que $F_{inf} \leq \expectation{F(w_k)}$ para todo $k \in \N$ vale:
	
	\begin{equation*}
	F_{inf} - \expectation{F(w_1)} \leq \expectation{F(w_{K+1})} - \expectation{F(w_1)} \leq  - \frac{1}{2}\mu \sum\limits_{1}^{K}{\alpha_k \expectation{\norm{\nabla F(w_k)}_2^2}} + \frac{1}{2} LM \sum\limits_{k=1}^{K} {\alpha_k^2}
	\end{equation*}
	
	Luego:
	
		\begin{equation*}
	\sum\limits_{1}^{K}{\alpha_k \expectation{\norm{\nabla F(w_k)}_2^2}}  \leq  \dfrac{2 (\expectation{F(w_1) - F_{inf} })}{\mu} + \frac{LM}{\mu}  \underbrace{\sum\limits_{k=1}^{K} {\alpha_k^2}}_{\underrightarrow{K \rightarrow \infty} \ C  \ < \ \infty }
	\end{equation*}
	
	\marginpar{\color{Red}Porque si $\lim \expectation{X_k} < \infty$ entonces $\lim \expectation{\frac{X_k}{A_k}} = 0$ si $A_k \rightarrow \infty$ con $A_k$ escalar ?}
	
	Por lo que \ref{eq: Objetivo no convexo, incremento decreciente, 1} esta probado. Finalmente como por \ref{eq: Condicion robbins monro} tenemos que $A_K \rightarrow \infty$ se tiene \ref{eq: Objetivo no convexo, incremento decreciente, 2}.\qed
	
\end{proof}

\begin{corollary}
	\label{coro: Gradientes cerca de cero, Objetivo no convexo, Incrementos decrecientes}
	Asumiendo \ref{hyp: F es L lipshitz} y \ref{hyp: Acotaciones momentos de g}, suponiendo adem\'as que en \ref{algo: DE} los $\sett{\alpha_k}$ satisfacen \ref{eq: Condicion robbins monro} entonces : 
	
	\begin{equation}
		\liminf\limits_{k \rightarrow \infty} {\expectation{\norm{\nabla F (w_k)}_2^2}} = 0 
	\end{equation}
	
\end{corollary}

\begin{corollary}
	Bajo las mismas hip\'otesis de \ref{theorem: Convergencia L1, Objetivo no convexo, Incrementos decrecientes} sea $k(K) \in \sett{1, \dots, K}$ un \'indice aleatorio elegido con probabilidades respectivas $\sett{\alpha_k}_{k=1}^K$; luego $\norm{\nabla F(w_k)}_2 \rightarrow 0$ en probabilidad.
\end{corollary}

\begin{proof}
	Sea $\epsilon > 0$, luego de $\ref{eq: Objetivo no convexo, incremento decreciente, 1}$ y la desigualdad de Markov:
	
	\marginpar{Usamos la desigualdad de Markov}
	
	\begin{equation*}
		\mathbb{P} \left[\norm{\nabla F (w_k)}_2 \geq \epsilon\right] = \mathbb{P} \left[\norm{\nabla F (w_k)}_2^2 \geq \epsilon^2\right] \leq \epsilon^{-2} \expectation{\expectationchik{\norm{\nabla F(w_k)}_2^2}}  \rightarrow 0
	\end{equation*}
	\marginpar{\color{Red} Porque aca es la esperanza de la esperanza condicional?} \qed
	
\end{proof}.

\begin{theorem}[Objetivo no convexo regular, Incrementos decrecientes]
	Bajo las mismas hip\'otesis de \ref{theorem: Convergencia L1, Objetivo no convexo, Incrementos decrecientes} si adem\'as pedimos que $F \in C^2$ y que $w \mapsto \norm{\nabla F (w)}_2^2$ sea $l-$ Lipshitz entonces:
	
	\begin{equation}
		\lim\limits_{k \rightarrow \infty} \expectation{\norm{\nabla F (w_k)}_2^2} = 0
	\end{equation}
	
\end{theorem}

\begin{proof}
	Sea $G(w) := \norm{\nabla F(w)}_2^2$ y sea $L_G$ la constante de Lipshitz de $\nabla G(w) = 2 \nabla^2 F(w) \nabla F(w)$, luego:
	
	\begin{equation*}
	\begin{aligned}
		G(w_{k+1}) - G(w_k) & \underbrace{\leq}_{\ref{obs: F es l lipshitz}} & \nabla G(w_k)^T (w_{k+1} - w_k) + \frac{1}{2} L_G \norm{w_k - w_{k+1}}_2^2 \\
		& \leq & - \alpha_k \nabla G(w_k)^T g(w_k, \upxi_{k}) + \frac{1}{2} \alpha_k L_G  \norm{g(w_k, \upxi_{k})}_2^2
	\end{aligned}
	\end{equation*}
	
	Si tomamos esperanza condicional a $\upxi_{k}$ y usamos \ref{hyp: F es L lipshitz}, \ref{hyp: Acotaciones momentos de g} entonces:
	
	\begin{equation*}
	\begin{aligned}
	\expectationchik{G(w_{k+1}) - G(w_k)} & \leq & -2 \alpha_k \nabla F(w_k)^T \nabla^2 F(w_k)^T \expectationchik{g(w_k, \upxi_{k})} + \\
	&& \frac{1}{2} \alpha_k^2 L_G \expectationchik{\norm{g(w_k, \upxi_{k})}^2_2} \\
	& \leq & 2 \alpha_k \norm{\nabla F(w_k)}_2 \norm{\nabla^2 F(w_k)}_2 \norm{\expectationchik{g(w_k, \upxi_{k})}}_2 +  \\
	&& \frac{1}{2} \alpha_k^2 L_G \expectationchik{\norm{g(w_k, \upxi_{k})}^2_2} \\
	& \leq & 2 \alpha_k L \mu_G \norm{\nabla F (w_k)}_2^2 + \\
	 && \frac{1}{2} \alpha_k^2 L_G\left(M + M_V \norm{\nabla F(w_k)}_2^2\right)
	\end{aligned}
	\end{equation*}
	
	Luego obtenemos tomando esperanza total:
	
	\begin{equation}
	\label{eq: Objetivo no convexo regular, Incrementos decrecientes}
	\expectation{G(w_{k+1})} - \expectation{G(w_k)} \leq 2 \alpha_k L \mu_G \expectation{\norm{\nabla F (w_k)}_2^2} +  \frac{1}{2} \alpha_k^2 L_G\left(M + M_V \expectation{\norm{\nabla F(w_k)}_2^2}\right)
	\end{equation}
	
	Notemos que existe $K\in \N$ tal que $\alpha_k^2 \leq \alpha_k$ y luego por \ref{theorem: Convergencia L1, Objetivo no convexo, Incrementos decrecientes} el lado derecho cumple:
	
	\begin{equation*}
	\lim\limits_{N \rightarrow \infty} { {2 L \mu_G \underbrace{\sum\limits_{k=K}^{K+N}\expectation{\alpha_k\norm{\nabla F (w_k)}_2^2}}_{\ref{eq: Objetivo no convexo, incremento decreciente, 1}} +  \frac{1}{2} L_G\left(M\underbrace{\sum\limits_{k=K}^{K+N}\alpha_k^2}_{\ref{eq: Condicion robbins monro}} + M_V \underbrace{\sum\limits_{k=K}^{K+N}\expectation{\alpha_k^2\norm{\nabla F(w_k)}_2^2}}_{{\ref{eq: Objetivo no convexo, incremento decreciente, 1}}}\right)} } = 0
	\end{equation*}
	
	Sean:
	
	\begin{equation*}
	\begin{aligned}
	S_K^+ & = & \sum\limits_{k=1}^{K} {\max\left(0, \expectation{G(w_{k+1})} - \expectation{G(w_k)} \right)} \\
	S_K^- & = & \sum\limits_{k=1}^{K} {\max\left(0, \expectation{G(w_{k})} - \expectation{G(w_{k+1})} \right)}
	\end{aligned}
	\end{equation*}
	
	Luego como en \ref{eq: Objetivo no convexo regular, Incrementos decrecientes} el lado derecho es positivo y su suma es convergente tenemos que $\sett{S_K^+}$ es mon\'otona, acotada superiormente y por ende convergente. Adem\'as como $0 \leq \expectation{G(w_k)} = \expectation{G(w_0)} + S_k^+ - S_k^-$ tenemos que $\sett{S_K^-}$ tambi\'en es mon\'otona y acotada superiormente, por lo que es convergente; conclu\'imos que $\expectation{G(w_k)}$ debe ser convergente, y por \ref{coro: Gradientes cerca de cero, Objetivo no convexo, Incrementos decrecientes} tenemos $\expectation{\norm{\nabla F(w_k)}_2^2} = \expectation{G(w_k)} \rightarrow 0$. \qed	
\end{proof}

