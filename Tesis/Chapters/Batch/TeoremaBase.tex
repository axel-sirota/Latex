\chapter{Teorema de la Variedad Estable y los Puntos fijos inestables}\label{ch:teorema-de-variedad-estable}

\epigraph{El matem\'atico que no tenga algo de poeta nunca podr\'a ser un matem\'atico completo.}{Karl Weierstrass}

\section{Intuici\'on}

Del cap\'itulo anterior ya sabemos que bajo condiciones de convexidad est\'andar el algoritmo \ref{algo: GD} converge puntualmente. Nos surge entonces la pregunta:

\medskip 

\textbf{Bajo qu\'e casos el algoritmo \ref{algo: GD - intro} converge (en alguna forma) con objetivos no convexos?}

\medskip

En el caso no convexon existen puntos extremales no \'optimos entre los cuales se encuentran los puntos silla, m\'aximos y m\'inimos locales "grandes" (\Eg  M\'inimos locales $w^*$ tales que $F_{inf} \ll F(w^*)$) [En la bibliograf\'ia a estos puntos se los llama \textit{shallow local minima}]. 

Los m\'aximos en general no son preocupantes pues la naturaleza misma de los algoritmos de primer orden \textit{escapa} de ellos. Intuitivamente esto es claro, ya que asumiendo suficiente regularidad, si tomamos un m\'aximo local $w^*$ y un entorno de \'este $B \ni w^*$ entonces como el gradiente apunta a la direcci\'on de m\'aximo crecimiento y este es en la direcci\'on a $w^*$, la iteraci\'on de descenso de gradiente cumplir\'a que  $ \norm{w^*-\alpha \nabla F(w_k)}^2$ es una sucesi\'on creciente en $B$.

\bigskip
Usemos un caso modelo para ejemplificar por qu\'e no es probable que los metodos de primer orden (entre ellos el algoritmo \ref{algo: GD}) convergan a puntos silla. Sea $f: \R^n \rightarrow \R^n$ dada por $f(x) = \dfrac{1}{2} x^THx$ con $H = \textbf{diag}\left(\lambda_1, \dots, \lambda_n\right)$; supongamos adem\'as que $\lambda_1, \dots, \lambda_k > 0$ y $\lambda_{k+1}, \dots, \lambda_n <0$ con $k<n$.

\begin{example}
	
	Si usamos en la base can\'onica de $\R^n, \ \mathcal{B} = \sett{e^1, \dots, e^n}$ entonces:
	
	\begin{equation*}
	f(x) = f(x^1, \dots, x^n) = \dfrac{1}{2} \left(\lambda_1 {x_1}^2 + \dots + \lambda_n {x_n}^2\right)
	\end{equation*}
	
	Por lo tanto:
	
	\begin{equation*}
	\nabla f (x) \ = \sum\limits_{i=1}^{n} \lambda_i x_i e^i = 0 \ \Longleftrightarrow \ x = 0
	\end{equation*}
	
	
	Y tenemos que en el \'unico punto cr\'itico $x=0$, el Hessiano de $f$ es $\nabla^2 f (0) =  H$.
	
	Recordemos que si $g(x) = x - \alpha  \nabla f(x)$ entonces el algoritmo \ref{algo: GD}  est\'a dado por la iteraci\'on $x_{t+1} = \ g(x_t) \ := g^t(x_0)$ con $t \in \N$ y $x_0 \in \R^n$, y en este caso esta representado por:
	
	\begin{equation*}
	\begin{aligned}
	x_{t+1} = & g(x_t) \\
	= & x_t - \alpha\nabla f(x_t) \\
	= & \sum\limits_{i=1}^{n} \left(1 - \alpha\lambda_i\right)(x_t)_i e_i \\
	= & \sum\limits_{i=1}^{n}\left(1 - \alpha\lambda_i\right)\ip{x_t, e_i} e_i
	\end{aligned}
	\end{equation*}
	
	Por lo tanto por inducci\'on es f\'acil probar que:
	
	
	\begin{equation*}
	x_{t+1} = \left(1 - \alpha\lambda_i\right)^t\ip{x_o, e^i}e^i
	\end{equation*}
	
	Sea $L = \max\limits_i\abs{\lambda_i}$ y supongamos que $\alpha < \dfrac{1}{L}$, luego:
	
	\begin{equation*}
	\begin{aligned}
	1 - \alpha \lambda_i < 1 & \quad \text{Si } i \leq k \\
	1 - \alpha \lambda_i > 1 & \quad \text{Si } i > k 
	\end{aligned}
	\end{equation*}
	
	Con lo que concluimos que:
	
	\begin{equation*}
	\lim\limits_t x_t = \left\lbrace{
		\begin{aligned}
		0 & \quad \text{Si } x \in E_s := gen\left\lbrace e^1, \dots, e^k \right\rbrace \\
		\infty & \quad \text{Si no} 
		\end{aligned}
	}\right.
	\end{equation*}
	
	Finalmente, si $k < n$ entonces concluimos que:
	
	\begin{equation*}
	\abs{\sett{x \in \R^n \ / \ \lim\limits_t g^t(x) = 0}} = \abs{E_s} = 0
	\end{equation*}
	
\end{example}

\medskip

\begin{example}
	
	Para notar este fen\'omeno en un ejemplo no cuadr\'atico, consideremos $f(x, y) = \frac{1}{2}x^2 + \frac{1}{4}y^4 - \frac{1}{2}y^2$, reproduciendo los calculos anteriores:
	
	\label{gradient descent ejemplo 2}
	\begin{equation}
	\begin{aligned}
	\nabla f & = & \left(x, y^3 -y\right) \\
	g & = & \left((1-\alpha)x, (1+\alpha)y - \alpha y^3\right) \\
	\nabla^2 f & = & \left(
	\begin{aligned}
	1 & \quad 0 \\
	0 & \quad 3y^2-1
	\end{aligned}
	\right) 
	\end{aligned}
	\end{equation}
	
	De lo que vemos que los puntos cr\'iticos son:
	
	\[
	z_1 \ = \ (0,0) \qquad z_2 \ = \ (0,1) \qquad z_3 \ = \ (0,-1)
	\]
	
	Y del criterio del Hessiano concluimos que $z_2, z_3$ son m\'inimos locales mientras que $z_1$ es un punto silla. De la intuici\'on previa, como en $z_1$ el autovector asociado al autovalor positivo es $e^1$ podemos intuir que:
	
	\begin{lemma}
		Para $f(x, y) = \frac{1}{2}x^2 + \frac{1}{4}y^4 - \frac{1}{2}y^2$ resulta que $E_s = \ip{ \left(t, 0, \dots, 0\right) \ / \ t \in \R}:= W_s$
	\end{lemma}
	
	Asumiendo el resultado por un momento, dado que $\dim_{\R^2}\left(E_s\right) =1 < 2$ entonces $\abs{E_s} = 0$ que es lo que quer\'iamos verificar. Demostremos el lema ahora:
	
	\begin{proof}{Del lema}
		Sea $x_0 \in \R^n$ y $g$ la iteraci\'on de \dg dada por \ref{gradient descent ejemplo 2}, luego:
		
		\begin{equation*}
		(x_t, y_t) \ = \ g^t(x,y) \ = \ \left(\begin{aligned}
		(1-\alpha)^tx_0 \\
		g_y^t(y_0)
		\end{aligned}\right) \ \substack{\longrightarrow \\ \left(t \rightarrow \infty\right)} \ \left(\begin{aligned}
		0 \\
		\lim\limits_t g_y^t(y_0)
		\end{aligned}\right)
		\end{equation*}
		
		Por lo que todo depende de $y_0$. Analizando $\dfrac{d g_y}{dy} = 1 + \alpha - 3\alpha y^2$ notemos que:
		
		\begin{equation*}
		\begin{aligned}
		\abs{\dfrac{d g_y}{dy}} < 1  & \Longleftrightarrow & \abs{1 + \alpha - 3\alpha y^2} < 1 \\
		& \Longleftrightarrow & -1 < 1 + \alpha - 3\alpha y^2 < 1 \\
		& \Longleftrightarrow &  -2 - \alpha < -3 \alpha y^2 < -\alpha \\
		& \Longleftrightarrow &  \sqrt{\dfrac{2 + \alpha}{3\alpha}} > \abs{y} > \sqrt{\dfrac{1}{3}} \\
		& \Longleftrightarrow &  \sqrt{\dfrac{1 + \frac{2}{\alpha}}{3}} > \abs{y} > \sqrt{\dfrac{1}{3}}
		\end{aligned}
		\end{equation*}
		
		Luego, bajo esas condiciones $g_y$ resulta una contracci\'on y como $\R$ es completo $g_y$ converge a un punto fijo $y_f$.
		
		\begin{equation*}
		\lim\limits_t g_y^t(y_0) = \left\lbrace \begin{aligned}
		y_f^1 & \qquad \text{Si } \sqrt{\dfrac{1 + \frac{2}{\alpha}}{3}} > y_0 > \sqrt{\dfrac{1}{3}} \\
		y_f^2 & \qquad \text{Si } \sqrt{\dfrac{1 + \frac{2}{\alpha}}{3}} < -y_0 < \sqrt{\dfrac{1}{3}}
		\end{aligned} \right.
		\end{equation*}
		
		Si analizamos simplemente los signos de $g$ y $\dfrac{d g_y}{dy}$ en los otros intervalos podemos conluir que:
		
		\begin{equation*}
		\lim\limits_t g_y^t(y_0) = \left\lbrace \begin{aligned}
		-\infty & \qquad \text{Si } y_0 >  \sqrt{\dfrac{1 + \frac{2}{\alpha}}{3}} \\
		y_f^1 & \qquad \text{Si } \sqrt{\dfrac{1 + \frac{2}{\alpha}}{3}} > y_0 > 0 \\
		y_f^2 & \qquad \text{Si } -\sqrt{\dfrac{1 + \frac{2}{\alpha}}{3}} < y_0 < 0 \\
		\infty & \qquad \text{Si } y_0 < -\sqrt{\dfrac{1 + \frac{2}{\alpha}}{3}} \\
		\end{aligned} \right.
		\end{equation*}
		
		Dedujimos entonces que $(x,y) \in E_s \ \Longleftrightarrow \ (x,y) = (t,0) \ t \in \R \ \Longleftrightarrow \ (x,y) \in W_s$. \qed
		
	\end{proof}
	
\end{example}

\section{Puntos fijos inestables}

Ahora que vimos un par de ejemplos que nos dan una intuici\'on acerca de la convergencia a puntos silla, usemos las herramientas de los sistemas din\'amicos para analizar el caso general.

Por todo esta secci\'on, $g: \mani \rightarrow \mani$ y $\mani$ es una $d-$variedad sin borde.

\begin{definition}
	Sea:
	\begin{equation*}
	\label{def:punto fijo inestable de g}
	\puntosfijos := \sett{x \ : \ g(x) = x \quad \max\limits_{i}\abs{\lambda_i \left(Dg(x)\right)} > 1}
	\end{equation*}
	
	el conjunto de puntos fijos de $g$ cuyo diferencial en ese punto tiene alg\'un autovalor mayor que $1$. A este conjunto lo llamaremos el conjunto de \textit{puntos fijos inestables}
	
\end{definition}

Con los resultados de \ref{ch:preliminares} demostremos el teorema principal para analizar la convergencia de los algoritmos de tipo batch en el caso no convexo:

\begin{theorem}
	\label{teo: Principal}
	Sea $g \in C^1(\mani)$ tal que $\det\left(Dg(x)\right) \neq 0$ para todo $x \in \mani$, luego el conjunto de puntos iniciales que convergen por $g$ a un punto fijo inestable tiene medida cero, \ie:
	
	\begin{equation*}
	\mu \left(\sett{x_0 \ \colon \ \Biglim{k}{g^k(x_0) \in \mathcal{A}_g^{*}}}\right) = 0
	\end{equation*}
\end{theorem}

\begin{proof}
	Sea $x^* \in \puntosfijos$, luego por el teorema de la funci\'on inversa existe $B_{x^*}$ un entorno abierto de $\mani$ tal que $g$ resulta difeomorfismo local; es m\'as, $\Bigcup{x^* \in \puntosfijos}{B_{x^{*}}}$ forma un cubrimiento abierto de $\puntosfijos$ del cual existe un subcubrimiento numerable pues $X$ es variedad, \ie
	
	\[
	\Bigcup{x^* \in \puntosfijos}{B_{x^{*}}} = \Bigcup{i \in \N}{B_{x^{*}_{i}}}
	\]
	
	Primero si $x_0 \in \mani$ sea:
	
	\begin{equation*}
	\begin{aligned}
	x_k = & g^k(x_0) \\
	= & \underbrace{g \circ \dots \circ g }_{k \ veces} (x_0)
	\end{aligned}
	\end{equation*}
	
	la sucesi\'on del flujo de $g$ evaluado en $x_0$, entonces si $W := \sett{x_0 \ \colon \ \Biglim{k}{x_k \in \mathcal{A}_g^{*}}}$ queremos ver que $\mu(W) = 0$.
	
	Sea $x_0 \in W$, luego como $x_k \rightarrow x^* \in \puntosfijos$ entonces existe $T \in \N$ tal que para todo $t \ge T$ , $x_t \in  \Bigcup{i \in \N}{B_{x^{*}_{i}}}$ por lo que $x_t \in B_{x^{*}_{i}}$ para alg\'un $x_{i}^{*} \in \puntosfijos$ y $t \ge T$. Afirmo que:
	
	\begin{lemma}
		\label{lemma: teo_principal}
		$x_t \in \Bigcap{k \in \N}{g^{-k}(B_{x_{i}^{*}})}$ para todo $t \ge T$
	\end{lemma}

	\begin{proof}{[Del lema]}
		Sea $k_0 \in \N$ y $\widetilde{t} \geq T$, luego $g^{k_0}\left(x_{\widetilde{t}}\right) = x_{\widetilde{t} + k_0} \in B_{x_{i}^{*}}$ pues $\widetilde{t} + k_0 \geq T$. Como $k_0$ era arbitrario y $\widetilde{t}$ era arbitrario sujeto a ser mayor que $T$ concluimos que $x_t \in \Bigcap{k \in \N}{g^{-k}(B_{x_{i}^{*}})}$ para todo $t \ge T$.\qed
	\end{proof}
	
	Si notamos $S_i \stackrel{\triangle}{=} \Bigcap{k \in \N}{g^{-k}(B_{x_{i}^{*}})}$, entonces por \ref{teo: variedad local estable central} sabemos por un lado que es una subvariedad de $W_{loc}^{cs}$ y por el otro que $\dim(S_i) \le \dim(W_{loc}^{cs}) = \dim(E_s) < d-1$; por lo que por \ref{prop: Dimension menor tiene medida 0} $\mu(S_i) = 0$.
	
	Finalmente como $x_{T} \in S_i$ para alg\'un $T$ entonces $x_0 \in \Bigcup{k \in \N}{g^{-k}(S_i)}$ por lo que $W \subseteq \Bigcup{i \in \N}{\Bigcup{k \in \N}{g^{-k}(S_i)}}$. concluimos:
	
	\begin{equation*}
	\begin{aligned}
	\mu \left(W\right) & \le & \mu \left(\Bigcup{i \in \N}{\Bigcup{k \in \N}{g^{-k}(S_i)}} \right) \\
	& \le & \Bigsum{i \in \N}{\Bigsum{k \in \N}{\mu \left(g^{-k}(S_i)\right)}} \\
	& \stackrel{\ref{Difeomorfismos locales preservan medida cero}}{=} & 0
	\end{aligned}
	\end{equation*}\qed
	
	
\end{proof}

Para finalizar veamos un caso simple que nos encontraremos seguido:

\begin{corollary}
	\label{coro: Resultado principal}
	Bajo las mismas hip\'otesis que en \ref{teo: Principal} si agregamos que $\mani* \subseteq \puntosfijos$ entonces $\mu(W_g) =0$
\end{corollary}

\begin{proof}
	Como $\mani^* \subseteq \puntosfijos$ entonces $W_g \subseteq W$, luego $\mu(W_g) \le \mu (W) \stackrel{\ref{teo: Principal}}{=} 0$.\qed
\end{proof}