\chapter{Convergencia CTP a m\'inimos : Caso general}\label{ch: aplicaciones}
\section{Descenso de Gradiente en Batch}

\epigraph{La diferencia entro los matem\'aticos y los f\'isicos es que luego de que los f\'isicos prueban un resultado grande piensan que es fant\'astico, pero luego que los matem\'aticos lo hacen piensan que es trivial.}{Richard Feynman}

Como una aplicaci\'on del teorema en \ref{teo: Principal} demostremos que el \textit{descenso de gradiente en batch} tiene probabilidad cero de converger a puntos silla. Consideremos el algoritmo \ref{algo: GD} con incrementos constantes $\alpha_k = \alpha$:

\begin{equation}
	\label{eq: GD}
	x_{k+1} = g(x_k) \stackrel{\triangle}{=} x_k - \alpha \nabla f(x_k)
\end{equation}

\begin{hyp}
	\label{hyp: Hessiano acotado}
	Asumamos que $f \in \mathcal{C}^2$ y $\norm{\nabla^2 f(x)}_2 \le L$
\end{hyp}

\begin{proposition}
	\label{prop: GD los puntos silla estrictos son fijos inestables}
	Todo punto silla estricto de $f$ es un punto fijo inestable de $g$, es decir $\mani^* \subseteq \puntosfijos$.
\end{proposition}

\begin{proof}
	Es claro que un punto cr\'itico de $f$ es punto fijo de $g$; si $x^* \in \mani^*$ entonces $Dg(x^*) = Id - \alpha \nabla^2 f(x^*)$ y entonces por \ref{prop: calculo funcional} los autovalores de $Dg$ son $\sett{1 - \alpha \lambda_i \tq \lambda_i \text{ autovalor de $\nabla^2 f$}}$. Como $x^* \in \mani^*$ existe $\lambda_{j^*} < 0$ por lo que $1 - \alpha\lambda_{j^*} >1$; concluimos que $x^{*} \in \puntosfijos$. \qed
	
\end{proof}

\begin{proposition}
	\label{prop: GD g es difeo local}
	Bajo \ref{hyp: Hessiano acotado} y $\alpha < \frac{1}{L}$ entonces $\det \left(Dg (x)\right) \neq 0$.
\end{proposition}

\begin{proof}
	Como ya sabemos $Dg(x) = Id - \alpha \nabla^2 f (x)$ por lo que:
	
	\begin{equation*}
		\det \left(Dg(x)\right) = \Bigprod{i \in \sett{1, \dots, d}}{\parenthesis{1 - \alpha \lambda_i}}
	\end{equation*}
	
	Luego por \ref{hyp: Hessiano acotado} tenemos que $\alpha < \frac{1}{\abs{\lambda_i}}$ y entonces $1 - \alpha \lambda_i > 0$ para todo $i \in \sett{1, \dots, d}$; concluimos que $\det \parenthesis{Dg(x)} > 0$. \qed
	
\end{proof}

\begin{corollary}
	\label{Dg converge a minimos}
	Sea $g$ dada por el algoritmo \ref{algo: GD}, bajo \ref{hyp: Hessiano acotado} y $\alpha < \frac{1}{L}$ se tiene que $\mu \left(W_g\right) = 0$.
\end{corollary}

\begin{proof}
	Por \ref{prop: GD los puntos silla estrictos son fijos inestables} y \ref{prop: GD g es difeo local} tenemos que vale \ref{coro: Resultado principal} y concluimos que $\mu \parenthesis{W_g} = 0$. \qed
\end{proof}

\section{Punto Pr\'oximo}

El algoritmo de punto pr\'oximo esta dado por la iteraci\'on:

\begin{equation}
\label{eq: Punto proximo}
x_{k+1} = g(x_k) \stackrel{\triangle}{=} \arg \min\limits_{z \in \mani}{f(z) + \dfrac{1}{2\alpha}\norm{x_k - z}_{2}^{2}}
\end{equation}

\begin{proposition}
	\label{prop: PP es difeo local y los puntos silla estrictos son fijos inestables}
	Bajo \ref{hyp: Hessiano acotado} y $\alpha < \frac{1}{L}$ entonces vale:
	
	\begin{enumerate}
		\item $\det \parenthesis{Dg(x)} \neq 0$
		\item $\mani^* \subseteq \puntosfijos$
	\end{enumerate}
	
\end{proposition}

\begin{proof}
	Veamos primero el siguiente lema:
	
	\begin{lemma}
		\label{lemma: PP es suave}
		Bajo \ref{hyp: Hessiano acotado}, $\alpha < \frac{1}{L}$ y $x \in \mani$ entonces $f(z) + \frac{1}{2 \alpha} \norm{x- z}_2^2$ es estrictamente convexa, por lo que $g \in \mathcal{C}^1 \parenthesis{\mani}$
	\end{lemma}

	\begin{proof}{[Del lema]}
		En efecto, por hip\'otesis sumado a \ref{prop: Implicancias L Lipschitz} y \ref{coro: convexa y fuertemente convexa es fuertemente convexa} concluimos que $g \in \mathcal{C}^1 \parenthesis{\mani}$.\qed
	\end{proof}
	
	Por lo tanto por \ref{lemma: PP es suave} podemos tomar l\'imite, \ie
	
	\begin{equation*}
	\begin{aligned}
	x_{k+1} & = & g(x_k) & = & \arg \min\limits_{z \in \mani}{f(z) + \dfrac{1}{2\alpha}\norm{x_k - z}_{2}^{2}} \\
	\downarrow & & \downarrow & & \downarrow \\
	x & = & g(x) & = & \arg \min\limits_{z \in \mani}{f(z) + \dfrac{1}{2\alpha}\norm{x - z}_{2}^{2}}
	\end{aligned}
	\end{equation*}
	\begin{equation*}
	\begin{aligned}
	\Longleftrightarrow & \nabla_z \parenthesis{f(z) + \frac{1}{2 \alpha} \norm{x-z}^2}(g(x)) = 0 \\
	\Longleftrightarrow & \nabla f(g(x)) - \frac{1}{\alpha} \parenthesis{x-g(x)} = 0 \\
	\Longleftrightarrow & g(x) + \alpha \nabla f(g(x)) = x
	\end{aligned}
	\end{equation*}
	
	Finalmente por diferenciaci\'on implicita obtenemos:
	
	\begin{gather*}
		Dg(x) + \alpha\nabla^2 f (g(x))Dg(x) = Id \\
		\Longrightarrow \quad Dg(x) = \parenthesis{Id + \alpha \nabla^2 f(g(x))}^{-1}
	\end{gather*}

	Luego si $x^* \in \mani^*$ entonces $Dg(x^*) = \parenthesis{Id + \alpha \nabla^2 f(x^*)}^{-1}$ y tiene autovalores $\sett{\dfrac{1}{1+ \alpha \lambda_i}}$ con $\lambda_i$ autovalores de $\nabla^2 f(x^*)$; luego como $\alpha < \frac{1}{L}$ se tiene que $\det \parenthesis{Dg(x)} \neq 0$. \qed
	
\end{proof}

\begin{corollary}
	\label{coro: PP converge a minimos}
	Sea $g$ dado por el algoritmo de punto pr\'oximo con ecuaci\'on \ref{eq: Punto proximo}, bajo \ref{hyp: Hessiano acotado} y $\alpha < \frac{1}{L}$ se tiene que $\mu\parenthesis{W_g} = 0$.
\end{corollary}

\begin{proof}
	Por \ref{prop: PP es difeo local y los puntos silla estrictos son fijos inestables} tenemos que vale \ref{coro: Resultado principal} y concluimos que $\mu \parenthesis{W_g} = 0$. \qed
\end{proof}

\section{Descenso por coordenadas}

Sea $S_1, \dots, S_b$ una partici\'on disjunta de $\sett{1, \dots, d}$ donde $d$ y $b$ son par\'ametros del m\'etodo, por ejemplo si $d =4$ y $b=2$ una posible opci\'on es $S_1 = \sett{1,3}$ y $S_2 = \sett{2,4}$.

Consideremos el algoritmo \ref{algo: CD}:

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[H]
	\caption{Descenso por coordenadas\label{algo: CD}}
	\textbf{Input:} $f \in C^1$, $\alpha >0$, $x_0 \in \mani$ \\
	\For{$k \in \N$}{
		\For{\textbf{block} $i = 1, \dots, b$}{
			\For{\textbf{index} $j \in S_i$}{
				$y_{k}^{S_0} = x_k$ e $y_{k}^{S_i} = \parenthesis{x_{k+1}^{S_1}, \dots, x_{k+1}^{S_i}, x_{k}^{S_{i+1}}, \dots, x_{k}^{S_b}}$ \\
				$x_{k+1}^{j} \gets x_{k}^j - \alpha \dfrac{\partial f}{\partial x_j}\parenthesis{y_{k}^{S_{i-1}}}$
			}
		}
	}
\end{algorithm}

Luego si definimos $g_i(x) = x - \alpha \Bigsum{j \in S_i}{e_j^T \nabla f(x)}$ entonces:

\begin{lemma}
	La iteraci\'on de Descenso por coordenadas esta dada por:
	
	\begin{equation}
		\label{eq: DC}
		x_{k+1} = g(x_k) \stackrel{\triangle}{=} g_b \circ g_{b-1} \circ \dots \circ g_1(x_k)
	\end{equation}
	
\end{lemma}

\begin{lemma}
	Si $g$ est\'a dada por la ecuaci\'on \ref{eq: DC} y $P_S = \Bigsum{i \in S}{e_ie_i^T}$ entonces:
	
	\begin{equation}
	\label{eq: Diferencial de DC}
	Dg(x_{k}) = \Bigprod{i \in \sett{1, \dots, b}}{\parenthesis{Id - \alpha P_{b-i+1}\nabla ^2 f(y_{k}^{S_{b-i}})}}
	\end{equation}
	
\end{lemma}

\begin{proof}
	Notemos primero que:
	
	\begin{equation*}
	Dg_{i}(x) = Id - \alpha P_{S_i} \nabla ^2 f(x)
	\end{equation*}
	
	Por lo tanto:
	
	\begin{equation*}
	\begin{aligned}[l]
	Dg(x_k) & = & D\parenthesis{g_b \circ \dots g_1}(x_k) \\
	& = & \parenthesis{Id - \alpha P_{S_b}\nabla^2f}\parenthesis{\underbrace{g_{b-1} \circ \dots g_{1}(x_k)}_{y_{k}^{S_{b-1}}}} D\parenthesis{g_{b-1} \circ \dots g_1}(x_k) \\
	& \vdots & \\
	& = & \Bigprod{i \in \sett{1, \dots, b}}{\parenthesis{Id - \alpha P_{b-i+1}\nabla ^2 f(y_{k}^{S_{b-i}})}}
	\end{aligned}
	\end{equation*}\qed
	
\end{proof}

\begin{hyp}
	\label{Hipotesis 2}
	Sea $f \in C^2$ y $\nabla^2 f \vert_S$ la submatriz compuesta de las filas y columnas indexadas por $S$. Por ejemplo si $f \in C^2(\R^3)$ y $S = \sett{1,2}$ entonces $\nabla^2 f \vert_S = \left(\begin{array}{cc}
	\dfrac{\partial^2 f}{\partial x_1 ^2} & \dfrac{\partial^2 f}{\partial x_1 x_2} \\
	\dfrac{\partial^2 f}{\partial x_2 x_1} & \dfrac{\partial^2 f}{\partial x_2 ^2}
	\end{array}\right)$
	
	Supongamos que $\max\limits_{i \in \sett{1, \dots, b}}{\norm{\nabla ^2 f(x) \vert_{S_i}}} = L_b$
\end{hyp}

\begin{proposition}
	\label{prop: DC es difeo local}
	Sea $f \in C^2$ y asumamos \ref{Hipotesis 2}, luego \ref{algo: CD} con $\alpha < \frac{1}{L_b}$ cumple que $\det(Dg(x)) \neq 0$
\end{proposition}

\begin{proof}
	Basta probar que cada t\'ermino de \ref{eq: Diferencial de DC} es invertible, para eso:
	\begin{equation*}
	\begin{aligned}
	\chi_{Dg_i(x)}(\lambda) & = & \det\parenthesis{
			\lambda Id_d - Id_d - \alpha P_{S_{b-i+1}}\nabla ^2 f(x)
		} \\
		& = & (\lambda -1)^{n-\abs{S_i}}\Bigprod{j \in S_i}{\parenthesis{\lambda -1 + \alpha\dfrac{\partial^2 f}{\partial x_j^2}(x) }}
	\end{aligned}
	\end{equation*}
	
	Luego si $\alpha < \frac{1}{L_{max}}$ entonces $\lambda -1 + \alpha\dfrac{\partial^2 f}{\partial x_j^2}(x) > 0$ para todo $j \in S_i, \ i \in \sett{1, \dots, b}$ por lo que todos los autovalores son positivos y $Dg_i(x)$ es invertible para todo $i$. \qed
	
\end{proof}

\begin{proposition}
	\label{prop: DC los puntos silla estrictos son fijos inestables}
	Bajo \ref{Hipotesis 2} y $\alpha < \frac{1}{L_{max}}$ se tiene que $\mani^* \subseteq \puntosfijos$
\end{proposition}

\begin{proof}
	Sea $x^* \in \mani^*$, $H = \nabla^2 f(x^*)$, 
	$J=Dg(x^*) = \Bigprod{i \leq b}{\left(Id_n - \alpha P_{S_{b-i+1}}H\right)}$ e $y_0$ el autovector correspondiente al menor autovalor de $H$. Vamos a probar que $\norm{J^t y_0}_2 \geq c(1+\epsilon)^t$ por lo que $\norm{J^t} _2 \geq  c(1+\epsilon)^t$, luego por \ref{prop: teorema de gelfand}:
	
	\begin{equation*}
	\rho(J) = \lim\limits_{t \rightarrow \infty}{\norm{J^t}^{1/t}} \geq \lim\limits_{t \rightarrow \infty}{c^{1/t} (1+\epsilon)} = 1 + \epsilon 
	\end{equation*}
	
	Y concluimos que $\mani^* \subseteq \puntosfijos$.
	
	En pos de eso fijemos $t \geq 1$ una iteraci\'on , $y_t = J^t x_0$, $z_1 = y_t$ y definamos $z_{i+1} = \parenthesis{Id - \alpha P_{S_i}H}z_i = z_i - \alpha \Bigsum{j \in S_i}{\parenthesis{e_j^T H z_i}e_j}$. Luego $y_{t+1} = z_{b+1}$, afirmo:
	
	\begin{claim}
		\label{claim1: DC los puntos silla estrictos son fijos inestables}
		Sea $y_t \in Ran(H)$, luego existe $i \in \sett{1, \dots, b}$ y $\delta > 0$ tal que $\alpha \Bigsum{j \in S_i}{\abs{e_j^T H z_i}} \geq \delta \norm{z_i}_2$
	\end{claim}
	
	\begin{lemma}
		\label{lemma1 : DC los puntos silla estrictos son fijos inestables}
		Existe $\epsilon >0$ tal que para todo $t \in \N$:
		
		\begin{equation*}
		\label{eq_1: DC los puntos silla estrictos son fijos inestables}
		y_{t+1}^THy_{t+1} \leq (1+\epsilon) y_{t}^THy_{t} 
		\end{equation*}
		
	\end{lemma}
	
	\begin{proof}
		Ver \ref{ch: apendice}
	\end{proof}

	Volviendo a la demostraci\'on general logramos probar que dado $y_0$ autovector de norma 1 de $H$ con menor autovalor $\lambda < 0$ (pues $x^* \in \mani^*$) vale que:
	
	\begin{equation*}
		\lambda_{min}(H) \norm{y_t}_2^2 \leq y_{t}^THy_{t} \leq \parenthesis{1+ \epsilon}^t y_{0}^THy_{0} \leq \parenthesis{1+ \epsilon}^t\lambda
	\end{equation*}

	Luego:
	
	\begin{equation*}
		\norm{y_t}_2^2 \geq \parenthesis{1+ \underbrace{\epsilon}{< \frac{1}{2}}}^{\frac{t}{2}}{\dfrac{\lambda}{\lambda_{min}(H)}} \geq {\dfrac{\lambda}{\lambda_{min}(H)}}\parenthesis{1 + \frac{\epsilon}{4}}^t
	\end{equation*}
	
	Que era lo que quer\'iamos demostrar con $c = \dfrac{\lambda}{\lambda_{min}(H)}$ y $\tilde{\epsilon} = \frac{\epsilon}{4}$. \qed

\end{proof}

\begin{corollary}
	\label{coro: DC converge a minimos}
	Sea $g$ dado por el algoritmo de descenso por coordenadas con ecuaci\'on \ref{eq: DC}, bajo \ref{Hipotesis 2} y $\alpha < \frac{1}{L_b}$ se tiene que $\mu\parenthesis{W_g} = 0$.
\end{corollary}

\begin{proof}
	Por \ref{prop: DC es difeo local} y \ref{prop: DC los puntos silla estrictos son fijos inestables} tenemos que vale \ref{coro: Resultado principal} y concluimos que $\mu \parenthesis{W_g} = 0$. \qed
\end{proof}
