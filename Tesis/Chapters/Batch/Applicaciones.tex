\chapter{Aplicaci\'ones}\label{ch: aplicaciones}
\section{Gradient Descent}

Como una aplicaci\'on del teorema en \ref{teo: Principal} demostremos que \dg tiene probabilidad cero de converger a puntos silla. Consideremos \dg con \textit{learning rate} $\alpha$:

\begin{equation}
	\label{eq: GD}
	x_{k+1} = g(x_k) \stackrel{\triangle}{=} x_k - \alpha \nabla f(x_k)
\end{equation}

\begin{remark}[Hip\'otesis 1]
	\label{Hipotesis 1}
	Asumamos que $f \in \mathcal{C}^2$ y $\norm{\nabla^2 f(x)}_2 \le L$
\end{remark}

\begin{proposition}
	\label{prop: GD los puntos silla estrictos son fijos inestables}
	Todo punto silla estricto de $f$ es un punto fijo inestable de $g$, \ie $\mani^* \subseteq \puntosfijos$.
\end{proposition}

\begin{proof}
	Es claro que un punto cr\'itico de $f$ es punto fijo de $g$; si $x^* \in \mani^*$ entonces $Dg(x^*) = Id - \alpha \nabla^2 f(x^*)$ y entonces los autovalores de $Dg$ son $\sett{1 - \alpha \lambda_i \tq \lambda_i \in \sett{\mu \tq \nabla^2 f(x^*)v = \mu v \quad \text{para alg\'un } v \neq 0}}$. Como $x^* \in \mani^*$ existe $\lambda_{j^*} < 0$ por lo que $1 - \alpha\lambda_{j^*} >1$; conclu\'imos que $x^{*} \in \puntosfijos$. \qed
	
	\marginpar{Usamos que f(A) tiene autovalores $f(\sett{\lambda_i})$}
	
\end{proof}

\begin{proposition}
	\label{prop: GD g es difeo local}
	Bajo \ref{Hipotesis 1} y $\alpha < \frac{1}{L}$ entonces $\det \left(Dg (x)\right) \neq 0$.
\end{proposition}

\begin{proof}
	Como ya sabemos $Dg(x) = Id - \alpha \nabla^2 f (x)$ por lo que:
	
	\begin{equation*}
		\det \left(Dg(x)\right) = \Bigprod{i \in \sett{1, \dots, d}}{\parenthesis{1 - \alpha \lambda_i}}
	\end{equation*}
	
	Luego por \ref{Hipotesis 1} tenemos que $\alpha < \frac{1}{\abs{\lambda_i}}$ y entonces $1 - \alpha \lambda_i > 0$ para todo $i \in \sett{1, \dots, d}$; conclu\'imos que $\det \parenthesis{Dg(x)} > 0$. \qed
	
\end{proof}

\begin{corollary}
	\label{Dg converge a minimos}
	Sea $g$ dada por \Dg en \ref{eq: GD}, bajo \ref{Hipotesis 1} y $\alpha < \frac{1}{L}$ se tiene que $\mu \left(W_g\right) = 0$.
\end{corollary}

\begin{proof}
	Por \ref{prop: GD los puntos silla estrictos son fijos inestables} y \ref{prop: GD g es difeo local} tenemos que vale \ref{coro: Resultado principal} y conclu\'imos que $\mu \parenthesis{W_g} = 0$. \qed
\end{proof}

\section{Punto Pr\'oximo}

El algoritmo de punto pr\'oximo esta dado por la iteraci\'on:

\begin{equation}
\label{eq: Punto proximo}
x_{k+1} = g(x_k) \stackrel{\triangle}{=} \arg \min\limits_{z \in \mani}{f(z) + \dfrac{1}{2\alpha}\norm{x_k - z}_{2}^{2}}
\end{equation}

\begin{proposition}
	\label{prop: PP es difeo local y los puntos silla estrictos son fijos inestables}
	Bajo \ref{Hipotesis 1} y $\alpha < \frac{1}{L}$ entonces vale:
	
	\begin{enumerate}
		\item $\det \parenthesis{Dg(x)} \neq 0$
		\item $\mani^* \subseteq \puntosfijos$
	\end{enumerate}
	
\end{proposition}

\begin{proof}
	Veamos primero el siguiente lema:
	
	\marginpar{Probamos esto? Me parece un poco claro}
	
	\begin{lemma}
		\label{lemma: PP es suave}
		Bajo \ref{Hipotesis 1}, $\alpha < \frac{1}{L}$ y $x \in \mani$ entonces $f(z) + \frac{1}{2 \alpha} \norm{x- z}_2^2$ es estrictamente convexa, por lo que $g \in \mathcal{C}^1 \parenthesis{\mani}$
	\end{lemma}
	
	Por lo tanto por \ref{lemma: PP es suave} podemos tomar l\'imite, \ie
	
	\begin{equation*}
	\begin{aligned}
	x_{k+1} & = & g(x_k) & = & \arg \min\limits_{z \in \mani}{f(z) + \dfrac{1}{2\alpha}\norm{x_k - z}_{2}^{2}} \\
	\downarrow & & \downarrow & & \downarrow \\
	x & = & g(x) & = & \arg \min\limits_{z \in \mani}{f(z) + \dfrac{1}{2\alpha}\norm{x - z}_{2}^{2}}
	\end{aligned}
	\end{equation*}
	\begin{equation*}
	\begin{aligned}
	\Longleftrightarrow & \nabla_z \parenthesis{f(z) + \frac{1}{2 \alpha} \norm{x-z}^2}(g(x)) = 0 \\
	\Longleftrightarrow & \nabla f(g(x)) - \frac{1}{\alpha} \parenthesis{x-g(x)} = 0 \\
	\Longleftrightarrow & g(x) + \alpha \nabla f(g(x)) = x
	\end{aligned}
	\end{equation*}
	
	Finalmente por diferenciaci\'on implicita obtenemos:
	
	\begin{gather*}
		Dg(x) + \alpha\nabla^2 f (g(x))Dg(x) = Id \\
		\Longrightarrow \quad Dg(x) = \parenthesis{Id + \alpha \nabla^2 f(g(x))}^{-1}
	\end{gather*}

	Luego si $x^* \in \mani^*$ entonces $Dg(x^*) = \parenthesis{Id + \alpha \nabla^2 f(x^*)}^{-1}$ y tiene autovalores $\sett{\dfrac{1}{1+ \alpha \lambda_i}}$ con $\lambda_i$ autovalores de $\nabla^2 f(x^*)$. Por lo tanto $x^* \in \puntosfijos$ y para $\alpha < \frac{1}{L}$ se tiene que $\det \parenthesis{Dg(x)} \neq 0$. \qed
	
\end{proof}

\begin{corollary}
	\label{coro: PP converge a minimos}
	Sea $g$ dado por el algoritmo de punto pr\'oximo con ecuaci\'on \ref{eq: Punto proximo}, bajo \ref{Hipotesis 1} y $\alpha < \frac{1}{L}$ se tiene que $\mu\parenthesis{W_g} = 0$.
\end{corollary}

\begin{proof}
	Por \ref{prop: PP es difeo local y los puntos silla estrictos son fijos inestables} tenemos que vale \ref{coro: Resultado principal} y conclu\'imos que $\mu \parenthesis{W_g} = 0$. \qed
\end{proof}

\section{Descenso por coordenadas}

Sea $S_1, \dots, S_b$ una partici\'on disjunta de $\sett{1, \dots, d}$ donde $d$ y $b$ son par\'ametros del m\'etodo. 

Consideremos el ritmo \ref{algo: CD}:

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[H]
	\caption{Descenso por coordenadas\label{algo: CD}}
	\textbf{Input:} $f \in C^1$, $\alpha >0$, $x_0 \in \mani$ \\
	\For{$k \in \N$}{
		\For{\textbf{block} $i = 1, \dots, b$}{
			\For{\textbf{index} $j \in S_i$}{
				$y_{k}^{S_0} = x_k$ e $y_{k}^{S_i} = \parenthesis{x_{k+1}^{S_1}, \dots, x_{k+1}^{S_i}, x_{k}^{S_{i+1}}, \dots, x_{k}^{S_b}}$ \\
				$x_{k+1}^{j} \gets x_{k}^j - \alpha \dfrac{\partial f}{\partial x_j}\parenthesis{y_{k}^{S_{i-1}}}$
			}
		}
	}
\end{algorithm}

Luego si definimos $g_i(x) = x - \alpha \Bigsum{j \in S_i}{e_j^T \nabla f(x)}$ entonces:

\begin{lemma}
	La iteraci\'on de Descenso por coordenadas esta dada por:
	
	\begin{equation}
		\label{eq: DC}
		x_{k+1} = g(x_k) \stackrel{\triangle}{=} g_d \circ g_{d-1} \circ \dots \circ g_1(x)
	\end{equation}
	
\end{lemma}

\begin{lemma}
	Si $g$ est\'a dada por \ref{eq: DC} entonces si notamos $P_S = \Bigsum{i \in S}{e_ie_i^T}$ entonces:
	
	\begin{equation}
	\label{eq: Diferencial de DC}
	Dg(x_{k}) = \Bigprod{i \in \sett{1, \dots, b}}{\parenthesis{Id - \alpha P_{b-i+1}\nabla ^2 f(y_{k}^{S_{b-i}})}}
	\end{equation}
	
\end{lemma}

\begin{proof}
	Notemos primero que:
	
	\begin{equation*}
	Dg_{i}(x) = Id - \alpha P_{S_i} \nabla ^2 f(x)
	\end{equation*}
	
	Por lo tanto:
	
	\begin{equation*}
	\begin{aligned}[l]
	Dg(x_k) & = & D\parenthesis{g_b \circ \dots g_1}(x_k) \\
	& = & \parenthesis{Id - \alpha P_{S_b}\nabla^2f}\parenthesis{\underbrace{g_{b-1} \circ \dots g_{1}(x_k)}_{y_{k}^{S_{b-1}}}} D\parenthesis{g_{b-1} \circ \dots g_1}(x_k) \\
	& \vdots & \\
	& = & \Bigprod{i \in \sett{1, \dots, b}}{\parenthesis{Id - \alpha P_{b-i+1}\nabla ^2 f(y_{k}^{S_{b-i}})}}
	\end{aligned}
	\end{equation*}\qed
	
\end{proof}

\begin{remark}
	\label{Hipotesis 2}
	Sea $f \in C^2$ y notemos $\nabla^2 f \vert_S$ a la submatriz que resulta de quedarme con filas y columnas indexadas por $S$. Sea $\max\limits_{i \in \sett{1, \dots, b}}{\norm{\nabla ^2 f(x) \vert_{S_i}}} = L_b$
\end{remark}

\begin{proposition}
	\label{prop: DC es difeo local}
	Bajo \ref{Hipotesis 2} y $\alpha < \frac{1}{L_b}$ se tiene que $\det(Dg(x)) \neq 0$
\end{proposition}

\begin{proof}
	Basta probar que cada t\'ermino de \ref{eq: Diferencial de DC} es invertible, para eso:
	\begin{equation*}
	\begin{aligned}
	\chi_{Dg_i(x)}(\lambda) & = & \det\parenthesis{
			\lambda Id_d - Id_d - \alpha P_{S_{b-i+1}}\nabla ^2 f(x)
		} \\
		& = & (\lambda -1)^{n-\abs{S_i}}\Bigprod{j \in S_i}{\parenthesis{\lambda -1 + \alpha\dfrac{\partial^2 f}{\partial x_j^2}(x) }}
	\end{aligned}
	\end{equation*}
	
	Luego si $\alpha < \frac{1}{L_{max}}$ entonces $\lambda -1 + \alpha\dfrac{\partial^2 f}{\partial x_j^2}(x) > 0$ para todo $j \in S_i, \ i \in \sett{1, \dots, b}$ por lo que todos los autovalores son positivos y $Dg_i(x)$ es invertible para todo $i$. \qed
	
\end{proof}

\begin{proposition}
	\label{prop: DC los puntos silla estrictos son fijos inestables}
	Bajo \ref{Hipotesis 2} y $\alpha < \frac{1}{L_{max}}$ se tiene que $\mani^* \subseteq \puntosfijos$
\end{proposition}

\begin{proof}
	Sea $x^* \in \mani^*$, $H = \nabla^2 f(x^*)$, 
	$J=Dg(x^*) = \Bigprod{i \leq b}{\left(Id_n - \alpha P_{S_{b-i+1}}H\right)}$ e $y_0$ el autovector correspondiente al menor autovalor de $H$. Vamos a probar que $\norm{J^t y_0}_2 \geq c(1+\epsilon)^t$ por lo que $\norm{J^t} _2 \geq  c(1+\epsilon)^t$, luego por el teorema de Gelfand
	
	\marginpar{Usamos que el radio espectral es el limite de cualquier norma matricial}
	
	\begin{equation*}
	\rho(J) = \lim\limits_{t \rightarrow \infty}{\norm{J^t}^{1/t}} \geq \lim\limits_{t \rightarrow \infty}{c^{1/t} (1+\epsilon)} = 1 + \epsilon 
	\end{equation*}
	
	Y conclu\'imos que $\mani^* \subseteq \puntosfijos$.
	
	En pos de eso fijemos $t \geq 1$ una iteraci\'on , $y_t = J^t x_0$, $z_1 = y_t$ y definamos $z_{i+1} = \parenthesis{Id - \alpha P_{S_i}H}z_i = z_i - \alpha \Bigsum{j \in S_i}{\parenthesis{e_j^T H z_i}e_j}$. Luego $y_{t+1} = z_{b+1}$, afirmo:
	
	\marginpar{Esta demo es horrenda, hay que pensar una mejor y pionerla en el Anexo}
	
	\begin{claim}
		\label{claim1: DC los puntos silla estrictos son fijos inestables}
		Sea $y_t \in Ran(H)$, luego existe $i \in \sett{1, \dots, b}$ y $\delta > 0$ tal que $\alpha \Bigsum{j \in S_i}{\abs{e_j^T H z_i}} \geq \delta \norm{z_i}_2$
	\end{claim}
	
	\begin{lemma}
		\label{lemma1 : DC los puntos silla estrictos son fijos inestables}
		Existe $\epsilon >0$ tal que para todo $t \in \N$:
		
		\begin{equation*}
		\label{eq_1: DC los puntos silla estrictos son fijos inestables}
		y_{t+1}^THy_{t+1} \leq (1+\epsilon) y_{t}^THy_{t} 
		\end{equation*}
		
	\end{lemma}
	
	\begin{proof}
		Manteniendo la notaci\'on previa a la afirmaci\'on:
		
		\begin{equation*}
		\begin{aligned}
			z_{i+1}^THz_{i+1} & \leq & \left[z_i^T - \alpha \Bigsum{j \in S_i}{\left( e_j^T H z_i \right) e_j^T} \right]H \left[ z_i - \alpha \Bigsum{j \in S_i}{\left(e_j^T H z_i \right) e_j }\right] \\
			& = & z_i^T H z_i - \alpha \Bigsum{j \in S_i}{\left(z_i^T H e_j\right)\left(e_j^T H z_i\right)} - \alpha \Bigsum{j \in S_i}{\left(e_j^T H z_i\right)\left(e_j^T H z_i\right)} \\
			&& + \alpha^2 \parenthesis{\Bigsum{j \in S_i}{\left(e_j^T H z_i\right)e_j}}^TH\parenthesis{\Bigsum{j \in S_i}{\left(e_j^T H z_i\right)e_j}} \\
			\parenthesis{\norm{H_{S_i}}_2 \leq L_b} & < & z_i^T H z_i -  2\alpha \Bigsum{j \in S_i}{\left(e_j^T H z_i\right)^2 } + \alpha ^2 L_b \norm{\Bigsum{j \in S_i}{\parenthesis{e_j^T H z_i}e_j}}^2_2 \\
		    & = & z_i^T H z_i -  \alpha \parenthesis{2 - \alpha L_b}\norm{\Bigsum{j \in S_i}{\parenthesis{e_j^T H z_i}e_j}}^2_2 \\
			\parenthesis{\alpha L_b < 1} & < & z_i^T H z_i -  \alpha \norm{\Bigsum{j \in S_i}{\parenthesis{e_j^T H z_i}e_j}}^2_2 
		\end{aligned} 
		\end{equation*}
		
		Luego juntando todo probamos que $z_i^T H z_i$ es decreciente y cumple la cota:
		
		\begin{equation}
		\label{eq_2: DC los puntos silla estrictos son fijos inestables}
			z_{i+1}^THz_{i+1}  < z_i^T H z_i -  \alpha \norm{\Bigsum{j \in S_i}{\parenthesis{e_j^T H z_i}e_j}}^2_2 
		\end{equation}
		
		Por otro lado sabemos que para todo $w$ vale:
		
		\begin{equation}
		\label{eq_3: DC los puntos silla estrictos son fijos inestables}
			w^THw  \geq  \lambda_{min}(H) \norm{w}_2^2 \geq -L_b \norm{w}^2_2
		\end{equation}
		
		\marginpar{Usamos Cauchy Schwartz}
		
		Luego si usamos \ref{claim1: DC los puntos silla estrictos son fijos inestables}, \ref{eq_3: DC los puntos silla estrictos son fijos inestables} y Cauchy-Schwartz existe $i \in \sett{1, \dots, b}$ y $\delta >0$ tal que:
		
		\begin{equation*}
		\begin{aligned}
			z_{i+1}^THz_{i+1} & < & z_i^T H z_i -  \alpha \Bigsum{j \in S_i}{\parenthesis{e_j^T H z_i}^2} \\
			& < &  z_i^T H z_i -  \frac{\alpha}{d} \parenthesis{\Bigsum{j \in S_i}{\abs{e_j^T H z_i}}}^2 \\
			& < &  z_i^T H z_i -  \dfrac{\delta^2}{d\alpha} \norm{z_i}_2^2\\
			& < &  \parenthesis{1 +  \dfrac{\delta^2}{d\alpha L_b}}z_i^T H z_i
		\end{aligned}
		\end{equation*}
		
		Tomando $\epsilon = \dfrac{\delta^2}{d\alpha L_b}$ probamos que $y_{t+1}^THy_{t+1} \leq (1+\epsilon) y_{t}^THy_{t} $ para $y_t \in Ran(H)$.
		
		Si $y_t = y_N + y_R$ con $y_N \in Ker(H)$, $y_R \in Ran(H)$ entonces $y_{t}^THy_{t} = y_{R}^THy_{R}$ y $y_{t+1} = Jy_t = y_N + Jy_R$ por lo que $y_{t+1}^THy_{t+1} = \parenthesis{Jy_{R}}^TH\parenthesis{Jy_{R}}$. Conclu\'imos:
		
		\begin{equation*}
			y_{t+1}^THy_{t+1} = \parenthesis{Jy_{R}}^TH\parenthesis{Jy_{R}} \leq (1+\epsilon) y_{R}^THy_{R} = (1+\epsilon) y_{t}^THy_{t}
		\end{equation*}
		\qed
	\end{proof}

	Volviendo a la demostraci\'on general logramos probar que dado $y_0$ autovector de norma 1 de $H$ con menor autovalor $\lambda < 0$ (pues $x^* \in \mani^*$) vale que:
	
	\begin{equation*}
		\lambda_{min}(H) \norm{y_t}_2^2 \leq y_{t}^THy_{t} \leq \parenthesis{1+ \epsilon}^t y_{0}^THy_{0} \leq \parenthesis{1+ \epsilon}^t\lambda
	\end{equation*}

	Luego:
	
	\begin{equation*}
		\norm{y_t}_2^2 \geq \parenthesis{1+ \underbrace{\epsilon}{< \frac{1}{2}}}^{\frac{t}{2}}{\dfrac{\lambda}{\lambda_{min}(H)}} \geq {\dfrac{\lambda}{\lambda_{min}(H)}}\parenthesis{1 + \frac{\epsilon}{4}}^t
	\end{equation*}
	
	Que era lo que quer\'iamos demostrar con $c = \dfrac{\lambda}{\lambda_{min}(H)}$ y $\tilde{\epsilon} = \frac{\epsilon}{4}$. \qed

\end{proof}

\begin{corollary}
	\label{coro: DC converge a minimos}
	Sea $g$ dado por el algoritmo de descenso por coordenadas con ecuaci\'on \ref{eq: DC}, bajo \ref{Hipotesis 2} y $\alpha < \frac{1}{L_b}$ se tiene que $\mu\parenthesis{W_g} = 0$.
\end{corollary}

\begin{proof}
	Por \ref{prop: DC es difeo local} y \ref{prop: DC los puntos silla estrictos son fijos inestables} tenemos que vale \ref{coro: Resultado principal} y conclu\'imos que $\mu \parenthesis{W_g} = 0$. \qed
\end{proof}
